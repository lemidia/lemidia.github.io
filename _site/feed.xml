<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2021-08-18T15:59:35+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Code Factory</title><subtitle>Deal with Computer science.</subtitle><author><name>Gyeong</name></author><entry><title type="html">Sequence to Sequence with Attention</title><link href="http://localhost:4000/development/boostcamp-week4-day18/" rel="alternate" type="text/html" title="Sequence to Sequence with Attention" /><published>2021-02-17T00:00:00+09:00</published><updated>2021-02-17T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week4-day18</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week4-day18/">&lt;h1 id=&quot;sequence-to-sequence&quot;&gt;Sequence to Sequence&lt;/h1&gt;

&lt;p&gt;Sequence를 Encoding과 Decoding 할 수 있는 sequence to sequence에 대해 알아봅니다.&lt;/p&gt;

&lt;p&gt;Sequence to sequence는 encoder와 decoder로 이루어져 있는 framework으로 대표적인 자연어 처리 architecture 중 하나입니다.&lt;/p&gt;

&lt;h2 id=&quot;encoder-decoder-architecture&quot;&gt;Encoder-decoder architecture&lt;/h2&gt;

&lt;p&gt;Sequence to Sequence 모델은 여러개의 입력 단어 시퀀스를 받아 처리하는 Encoder, 출력 단어를 순차적으로 생성하는 Decoder로 구성됩니다.&lt;/p&gt;

&lt;p&gt;각각의 모듈은 서로 파라미터를 공유하지 않는 독립된 RNN 모듈이라고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;그림에서는 RNN 모델을 LSTM 모듈로서 채용한 것을 알 수 있습니다.&lt;/p&gt;

&lt;p&gt;Sequence to Sequence의 Encoder에서 입력 단어의 마지막 까지 읽어들인 후, Encoder에서 나온 제일 마지막 hidden state vector가 Decoder로 제일 처음 h0 벡터로서 입력으로 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;h0는 입력 단어들을 잘 요약 정리한 정보라고 할 수 있고, 이는 출력단에서 순차적으로 처리가 되어 대응 되는 다음 단어를 생성하는데 사용될 수 있습니다.&lt;/p&gt;

&lt;p&gt;Start 토큰은 실질적으로 예측하고자 하는 결과 단어를 만들기 위한 제일 처음 넣어주는 인풋값으로서 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;End 토큰은 Decoder에서 문장이 끝나는 시점을 의미하고, 이 토큰이 생성되면 디코더의 실행은 멈추게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;seq2seq-model-with-attention&quot;&gt;Seq2Seq Model with Attention&lt;/h2&gt;

&lt;p&gt;기존에는 인코더의 마지막 hidden state 벡터의 입력만 디코더에 넣어줘서 결과를 순차적으로 생성했다면, Attention을 적용한 경우에는, 인코더에서 각각의 타임스탭마다 만들어진 중간 노드들의 hidden state vector들을 디코더의 매 타임 스탭마다 하나의 정보로서 같이 활용하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;인코더에서 만들어진 마지막 hidden state vector = h0와 start 토큰이 디코더를 거쳐 h1 state vector를 만들게 되는데 이는 인코더 단의 각각의 매 타임 스탭마다 만들어진 h1 ~ h4와 각각 내적의 연산을 수행하게 됩니다.&lt;/p&gt;

&lt;p&gt;이 내적 연산의 결과값의 의미는 h1 state vector와 각각의 인코더 hidden state vector간의 내적에 기반한 유사도라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리는 이 값들을 다시 소프트맥스 층에 적용하여, 각각의 인코더 hidden state vector와의 대응되는 확률 값으로 해석할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이 각각의 값들은, 각각의 인코더 hidden state vector로의 부여되는 가중치로서 사용이 되고, 이 4개의 값들을 가중평균을 내어 다시 종합된 하나의 인코딩 vector를 생성할 수 있게 됩니다. 이를 context vector 라고도 부릅니다.&lt;/p&gt;

&lt;p&gt;예시로든 사진에서는 0.85의 값이 제일 높은데 이는, 인코더의 첫번째 타임스탭의 단어를 제일 높은 확률로 필요로 한다는 의미가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;밑의 그림에서 녹색 원 안에의 부분을 우리는 Attention 모듈이라고 부르게 되고, 입력으로 인코더 단에서의 각각의 hidden state vector와 디코더 단에서의 hidden state vector가 들어가게 되고, Attention 모듈의 출력으로는 인코더 단에서의 각각의 hidden state vector의 가중 평균이 되는 context vector가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 만들어진 context vector는 앞서 만들어진 디코더 단의 hidden state vector와 concat이 되어서 그림에서는 y3 아웃풋을 만들게 되고, 이는 다음 타임 스탭으로의 입력으로 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;디코더 단에서 y6인 money를 예측하는 타임 스탭에서의 그림은 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day18-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;이렇게 디코더 단에서 매 타임 스탭마다 단어를 예측하는데에 있어서 각각의 인코더 hidden state vector가 쓰이게 되고 매번 다른 가중치가 적용된 가중평균된 context vector를 생성하게 되고, context vector가 매 타임 스탭마다 생성되고 아웃풋에 직접적인 입력의 일부로서 사용이 되어 그 해당 타임 스탭에서의 보다 좀 더 정확한 예측이 가능하도록 하게 만듭니다.&lt;/p&gt;

&lt;p&gt;이를 End of sentence를 의미하는 END 토큰이 나올 때 까지 반복을 하게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN - 주재걸, 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="RNN" /><category term="Python" /><category term="Math" /></entry><entry><title type="html">Recurrent Neural Network, Types of RNNs and LSTM</title><link href="http://localhost:4000/development/boostcamp-week4-day17/" rel="alternate" type="text/html" title="Recurrent Neural Network, Types of RNNs and LSTM " /><published>2021-02-16T00:00:00+09:00</published><updated>2021-02-16T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week4-day17</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week4-day17/">&lt;h1 id=&quot;sequential-models---rnn&quot;&gt;Sequential Models - RNN&lt;/h1&gt;

&lt;p&gt;자연어 처리 분야에서 Recurrent Neural Network(RNN)가 무엇이고, Gradient Vanishing/Exploding의 문제를 보완한 Vanilla RNN을 발전시킨 LSTM을 알아보고, RNN의 Type을 알아봅니다.&lt;/p&gt;

&lt;h2 id=&quot;sequential-model---naive-sequence-model&quot;&gt;Sequential Model - Naive sequence model&lt;/h2&gt;

&lt;p&gt;Sequential Data는 우리의 일상에서 언어나 주식 차트, 영상 등이 해당될 수 있습니다.&lt;/p&gt;

&lt;p&gt;이미지 같은 고정된 차원의 데이터가 아닌 시간에 따라 데이터가 입력이 되는 시계열 데이터에 대해 동작하는 것이 Sequential Model이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어 현재 무슨 언어가 나올지 예측하는 모델을 생각해 볼 수 있습니다. 시간상으로 전에 나온 단어들로 미래의 단어를 예측한다던지..&lt;/p&gt;

&lt;p&gt;첫번째 단어가 입력이 되면, 두번째 단어는 첫번째 단어를 고려, 세번째 단어는 두번째, 첫번째 단어를 고려.. 반복&lt;/p&gt;

&lt;p&gt;그럼 이 모델은 현재 단어를 예측하기 위해 고려해야 하는 데이터의 개수가 시간이 지남에 따라 계속 증가하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sequential-model---auto-regressive-model&quot;&gt;Sequential Model - Auto regressive model&lt;/h2&gt;

&lt;p&gt;위의 모델은 현재를 예측할 때 과거의 데이터를 다 고려했다면, 이 방법은 fixed timespan을 두어 과거의 몇개만 고려하는 것입니다.&lt;/p&gt;

&lt;p&gt;예를들어 과거에 대해 5개의 데이터만 보겠다하면 T의 값은 5가 됩니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;x_t = x_1 ~ x_5&lt;/code&gt;까지만 보겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;markov-model&quot;&gt;Markov model&lt;/h2&gt;

&lt;p&gt;이 모델은 현재의 값은 직전 과거의 하나의 데이터에만 dependent 하다는 것을 가정으로 두게됩니다.&lt;/p&gt;

&lt;p&gt;이 모델은 과거의 하나의 데이터만을 이용하기 때문에, 위의 모델에 비해 당연히 많은 과거의 정보를 이용할 수가 없게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;latent-autoregressive-model&quot;&gt;Latent autoregressive model&lt;/h2&gt;

&lt;p&gt;처음 살펴본 두개의 모델의 가장 큰 단점은 과거의 방대한 데이터를 다 고려해야한다는 것입니다.&lt;/p&gt;

&lt;p&gt;이 모델은 모델 중간에 Hidden state가 들어있는 모델입니다. 이 Hidden state가 하는 일은 과거 데이터의 정보를 summary 한다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;그래서 다음의 타임스텝의 아웃풋은 현재의 인풋과 바로 이전의 Hidden state에 의존하여 작동할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;/h2&gt;

&lt;p&gt;위에서 설명한 여러가지 모델들을 가장 잘 표현한 것이 Recurrent Neural Network 라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Recurrent Neural Network는 MLP와 다른 점이 있다면, 자기 자신을 돌아오는 구조가 하나 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;이런 구조에서 타임t에서의 h는 x_t에서만 dependent한 것이 아니라, t-1의 A로 표시된 Sell state에도 dependent 하게 됩니다.&lt;/p&gt;

&lt;p&gt;밑의 그림에서 오른쪽 그림은 왼쪽에서의 그림을 시간순으로 쭉 나열한 것이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;short-term-dependencies&quot;&gt;Short-term dependencies&lt;/h3&gt;

&lt;p&gt;위에서 살펴본 RNN 구조에서의 가장 큰 단점이라고 한다면 Short-term dependencies을 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;과거의 모든 정보들이 다 취합이 되서 요약되고 미래에서 그것을 고려가 되어야하는데, RNN 자체는 하나의 fixed rule로 이 정보들을 계속 취합하기 때문에, 먼 과거에 있던 정보가 미래까지 살아남기 힘든 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;Short-term dependencies 즉, 현재에서 몇개의 전 과거의 데이터는 잘 고려가 되는데, 한 참 멀리 있는 정보를 고려하기 힘든것을 말하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory&quot;&gt;Long Short-term Memory&lt;/h3&gt;

&lt;p&gt;기존의 RNN 구조를 살펴보면, x가 A의 구조를 통과해서 h라는 아웃풋을 산출하고 h는 다시 다음번의 A구조로 들어가게 됩니다. 여기서 A구조에서는 weight와 계산이 되고, activation function과 계산이되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 RNN을 개선한 Long Short-term Memory의 구조는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;Long Short-term Memory의 각각의 컴포넌트가 어떻게 동작하고 이 구조가 왜 Short-term dependencies를 극복하는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Long Short-term Memory의 구조를 세부적으로 묘사한 그림입니다.&lt;/p&gt;

&lt;p&gt;x_t는 인풋으로써 언어 모델이라면 여러개의 단어(5000개의 단어)를 표현한 원-핫 벡터가 될 수도 있고, 워드 임베딩된 벡터로 표현될 수도 있습니다.&lt;/p&gt;

&lt;p&gt;h_t는 hidden state이면서, output이 됩니다.&lt;/p&gt;

&lt;p&gt;previous cell state는 0부터 t-1까지의 정보를 다 취합해서 summarize 해주는 정보가 됩니다.&lt;/p&gt;

&lt;p&gt;previous hidden state는 위쪽으로 previous cell state 들어가기도 하지만 오른쪽으로 여러 연산의 입력으로도 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;전체적으로 보면 들어오는 입력은 3개가 되고 나가는 출력도 3개가 됩니다. 출력 중 2개는 다음번 입력으로 들어가고, 나머지 하나는 실제 아웃풋이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Long Short-term Memory은 총 4개의 gate로 이루어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 검은 선이 바로 cell state이고, C_0부터 C_t 까지 들어온 정보를 요약하게 됩니다.&lt;br /&gt;
타임스탬프 t마다 컨베이어 벨트에서 처럼 물건(정보)이 올라오고, 그 정보들을 잘 조작해서, 어떤 것이 유용하고 어떤 것이 유용하지 않은지 정해서 다음번 셀로 이 정보들을 넘겨주는 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;정보들을 잘 조작해서, 어떤 것이 유용하고 어떤 것이 유용하지 않은지 정해서 올리는 역할은 gate가 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Forget gate는 어떤 정보를 버릴지 정하는 역할을 합니다.&lt;br /&gt;
Forget gate에는 현재의 입력 x_t와 이전의 hidden state value인 h_t-1이 들어가서 f_t라는 숫자를 얻어내게 됩니다. 시그모이드를 통과하기 때문에 항상 f_t는 0에서 1사이 값을 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;f_t는 이전 cell state에서 나온 정보들 중에 어떤 것을 버리고, 어떤것을 살릴지 정해주게 됩니다.&lt;/p&gt;

&lt;p&gt;Input gate는 어떤 정보를 cell state에 저장할지(올릴지) 정하는 역할을 합니다.&lt;br /&gt;
현재에 어떤 입력이 들어왔는데, 이것을 무작정 cell state에 올리는 것이 아닌, 이 정보중에 어떤 정보를 올릴지 정하게 됩니다.&lt;/p&gt;

&lt;p&gt;i_t는 어떤 정보를 cell state에 올릴지 또는 저장할지 정하게 됩니다. 이전의 hidden state value와 현재의 입력을 가지고 만들어 지게 됩니다.&lt;/p&gt;

&lt;p&gt;그렇다면 우리가 실제로 올릴 정보를 알아야합니다.&lt;/p&gt;

&lt;p&gt;C_t~가 이 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;update gate는 이전까지 summarize 되어있던 cell state와 올릴지 또는 저장할지 정하는 i_t 그리고 C_t~를 가지고 cell state에 update하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;이전 cell state인 C_t-1과 f_t 만큼을 곱해서 C_t-1에서 어떤 정보를 버리게 되고 나오는 값 A, C_t~와 i_t만큼을 곱해서 어느 값을 올릴지를 정해서 만들어진 값 B, 이 두 값을 combine해서 다음 cell state인 C_t으로 업데이트 하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;Out state는 어떤 값을 밖으로 내보낼지 결정하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;o_t는 어떤 값을 내보낼지 정하는 역할을 하게 되고 이를 thanh를 통과한 C_t인 updated cell state와 Element wise multiplication을 해서 결정적으로 h_t가 만들어 지게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;types-of-rnns&quot;&gt;Types Of RNNs&lt;/h2&gt;

&lt;p&gt;다루고자 하는 데이터와 문제에 따라 RNN을 여러 형태로 하여 구성할 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;one-to-one&lt;/li&gt;
  &lt;li&gt;one-to-many&lt;/li&gt;
  &lt;li&gt;many-to-one&lt;/li&gt;
  &lt;li&gt;many-to-many&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;one-to-one&quot;&gt;One-to-one&lt;/h3&gt;

&lt;p&gt;입력과 출력의 타임스탭이 단 하나인 스탠다드한 모델 구조를 말합니다.&lt;br /&gt;
모델은 입력 벡터를 받아 가중치를 이용해 입력 벡터를 선형변환과 필요하다면 비선형 함수를 적용하고, 출력 벡터를 산출한다는 의미가 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day17-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;one-to-many&quot;&gt;One-to-many&lt;/h3&gt;

&lt;p&gt;입력이 하나의 타임스탭으로 이루어지고, 출력은 여러 타임스탭으로 이루어진 모델 구조를 말합니다.&lt;br /&gt;
대표적으로 Image Captioning이 이에 해당할 수 있습니다. 입력으로는 하나의 이미지를 받고, 이 이미지에 대한 설명으로 각 타임스탭마다 하나의 단어를 산출한다는 점을 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;매 타입스탭마다 모델로 입력이 주어져야 하지만, 밑의 사진처럼 따로 주어질 입력이 없는 경우 원래의 입력 사이즈와 같고 값이 0으로 채워진 벡터나 행렬을 입력으로 주게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day17-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;many-to-one&quot;&gt;Many-to-one&lt;/h3&gt;

&lt;p&gt;입력이 여러개의 타임스탭으로 이루어지고, 출력은 하나의 타임스탭으로 이루어진 모델을 말합니다.&lt;br /&gt;
대표적으로 여러개의 입력을 받아 종합적으로 이 입력들의 감정을 분류하는 Sentiment Classification을 예로 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day17-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;many-to-many&quot;&gt;Many-to-many&lt;/h3&gt;

&lt;p&gt;입력과 출력 모두 여러개의 타임스탭으로 이루어진 시퀀스한 모델을 말합니다.&lt;br /&gt;
대표적으로 입력 단에서 특정 어느 나라 말을 끝까지 시간순으로 읽은 후, 출력단에서 순차적으로 다른 나라말로 번역(예측)이 되는, 기계독해 - Machine Translation을 예로 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day17-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN - 주재걸, 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="RNN" /><category term="Python" /><category term="Math" /></entry><entry><title type="html">Intro to NLP, Bag-of-Words, Naive Bayes Classifier</title><link href="http://localhost:4000/development/boostcamp-week4-day16/" rel="alternate" type="text/html" title="Intro to NLP, Bag-of-Words, Naive Bayes Classifier " /><published>2021-02-15T00:00:00+09:00</published><updated>2021-02-16T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week4-day16</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week4-day16/">&lt;h1 id=&quot;intro-to-nlp-bag-of-words-naive-bayes-classifier&quot;&gt;Intro to NLP, Bag-of-Words, Naive Bayes Classifier&lt;/h1&gt;

&lt;p&gt;NLP에 대해 짧게 소개하고 자연어를 처리하는 가장 간단한 모델 중 하나인 &lt;strong&gt;Bag-of-Words&lt;/strong&gt;를 알아봅니다.&lt;/p&gt;

&lt;p&gt;Bag-of-Words는 단어의 표현에 있어서 one-hot-encoding을 이용하며, 단어의 등장 순서를 고려하지 않는 아주 간단한 방법 중 하나입니다. 간단한 모델이지만 많은 자연어 처리 task에서 효과적으로 동작하는 알고리즘 중 하나입니다.&lt;/p&gt;

&lt;p&gt;그리고, 이 Bag-of-Words를 이용해 문서를 분류하는 &lt;strong&gt;Naive Bayes Classifier&lt;/strong&gt;에 대해서 설명합니다.&lt;/p&gt;

&lt;h2 id=&quot;what-is-natural-language-processing-nlp&quot;&gt;What is Natural language processing? (NLP)&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Natural language processing (NLP)&lt;/strong&gt;, which aims at properly understanding and generating human languages, emerges as a crucial application of artificial intelligence, with the advancements of deep neural networks.&lt;/p&gt;

&lt;p&gt;인간의 언어를 적절하게 이해하고 생성하는 것을 목표로하는 자연어 처리 (NLP)는 심층 신경망의 발전과 함께 인공 지능의 중요한 응용 프로그램으로 부상하고 있습니다.&lt;/p&gt;

&lt;p&gt;NLP is used in various field in deep learning approaches as well as their applications such as language modeling, machine translation, question answering, document classification, and dialog systems.&lt;/p&gt;

&lt;p&gt;NLP는 언어 모델링, 기계 번역, 질문 답변, 문서 분류 및 대화 시스템과 같은 응용 프로그램뿐만 아니라 딥 러닝 접근 방식의 다양한 분야에서 사용됩니다.&lt;/p&gt;

&lt;p&gt;Natural language processing의 연구동향은 ACL, EMNLP, NAACL와 같은 학회에서 발표가 됩니다.&lt;/p&gt;

&lt;h3 id=&quot;academic-disciplines-related-to-nlp&quot;&gt;Academic Disciplines related to NLP&lt;/h3&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Natural language processing (major conferences: ACL, EMNLP, NAACL)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Low-level parsing : Tokenization(문자를 특정 단위로 자르는 것), stemming(영어, 한글에서와 같이 같은 의미론적의 단어라도 어미나 형태가 달라질 수 있고, 여기서 어근을 뽑아내는 것 ex) study, studied, studying…)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Word and phrase level : Named entity recognition(NER), part-of-speech(POS) tagging, noun-phrase chunking, dependency parsing, coreference resolution&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sentence level : Sentiment analysis(감정 분석), machine translation(기계 독해)&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Multi-sentence and paragraph level : Entailment prediction, question answering(질문에 대한 정확한 대답), dialog systems(챗봇), summarization(긴 문장이나 문단에 대해서 짧게 요약)&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Text mining (major conferences: KDD, The WebConf (formerly, WWW), WSDM, CIKM, ICWSM)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Extract useful information and insights from text and document data : 예를 들어, 방대한 뉴스 데이터로 부터 현재 AI 트렌드 키워드를 분석하는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Document clustering (e.g., topic modeling) : 방대한 데이터로 부터 여러 다른 주제에 대해 서로 그룹화 하는 것&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Highly related to computational social science : 예를 들어, 소셜 미디어 데이터에 기반해 사람들의 정치적 성향의 변화를 분석하는 것&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Information retrieval (major conferences: SIGIR, WSDM, CIKM, RecSys)&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Recommendation system : 추천 시스템 (사용자의 성향을 바탕으로 관련된 토픽을 추천해 주는 등…)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;trends-of-nlp&quot;&gt;Trends of NLP&lt;/h2&gt;

&lt;p&gt;Text data can basically be viewed as a sequence of words, and &lt;strong&gt;each word can be represented as a vector&lt;/strong&gt; through a technique such as Word2Vec or GloVe.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;RNN-family models&lt;/strong&gt; (LSTMs and GRUs), which take the sequence of these vectors of words as input, are the main architecture of NLP tasks.&lt;/p&gt;

&lt;p&gt;Overall performance of NLP tasks has been improved since &lt;strong&gt;attention modules and Transformer models&lt;/strong&gt;, which replaced RNNs with self-attention, have been introduced a few years ago.&lt;/p&gt;

&lt;p&gt;As is the case for Transformer models, most of the advanced NLP models have been originally developed for improving machine translation tasks.&lt;/p&gt;

&lt;p&gt;In the early days, &lt;strong&gt;customized models for different NLP tasks&lt;/strong&gt; had developed separately.&lt;/p&gt;

&lt;p&gt;Since Transformer was introduced, huge models were released by stacking its basic module, self-attention, and these models are trained with large-sized datasets through language modeling tasks, one of the &lt;strong&gt;self-supervised training setting that does not require additional labels&lt;/strong&gt; for a particular task.&lt;/p&gt;

&lt;p&gt;Afterwards, above models were applied to other tasks through &lt;strong&gt;transfer learning&lt;/strong&gt;, and they outperformed all other customized models in each task.&lt;/p&gt;

&lt;p&gt;Currently, these models has now become essential part in numerous NLP tasks, so NLP
research become difficult with limited GPU resources, since they are too large to train.&lt;/p&gt;

&lt;h2 id=&quot;bag-of-words&quot;&gt;Bag-of-Words&lt;/h2&gt;

&lt;p&gt;Bag-of-Words는 단어의 표현에 있어서 숫자 형태로 나타내는 one-hot-encoding을 이용하며, 단어의 등장 순서를 고려하지 않는 아주 간단한 방법 중 하나입니다. 간단한 모델이지만 많은 자연어 처리 task에서 효과적으로 동작하는 알고리즘 중 하나입니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 1. Constructing the vocabulary containing unique words&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;문장들에서, 고유한 단어들을 뽑아서 단어사전을 구축합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 2. Encoding unique words to one-hot vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;고유한 단어들을 순서대로 원-핫-인코딩을 적용합니다.&lt;/p&gt;

&lt;p&gt;항상 자기 자신을 제외한 어느 단어들과의 유클리드 거리는 루트 2, 코사인 유사도(내적)은 0이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Step 3. A sentence/document can be represented as the sum of one-hot vectors&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;sentence/document를 이뤘던 단어들의 원-핫 벡터를 Element wise하게 모두 더해줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;naivebayes-classifier-for-document-classification&quot;&gt;NaiveBayes Classifier for Document Classification&lt;/h2&gt;

&lt;p&gt;Bag-of-Words로 나타내어진 문서를 정해진 카테고리 혹은 클래스로 분류할 수 있는 모델입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Bag-of-Words for Document Classification&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Bayes’ Rule Applied to Documents and Classes&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;d = 분류 되어질 문서, C = C개의 클래스&lt;/p&gt;

&lt;p&gt;특정한 문서 d가 주어져 있을 때, 그 문서가 특정 클래스 중 하나인 c에 속할 확률 : P(c|d), 조건부 확률 분포상, 가장 높은 확률을 가지는 c를 통해서 문서 분류를 수행&lt;/p&gt;

&lt;p&gt;P(c|d)는 Bayes’ Rule에 의해 아래의 그림에서의 중간 식과 같이 나타내어 질 수 있다.&lt;/p&gt;

&lt;p&gt;Bayes’ Rule에서 P(d)는 문서 d가 뽑힐 확률, d는 우리가 분류할 고정된 하나의 개체라고 볼 수 있으므로, argmax operation 상에서 상수로 무시할 수 있게 되고, 이는 맨 아래의 식으로 유도될 수 있음.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;P(d|c)는 특정 클래스가 고정이 되었을 때, 문서 d가 나타날 확률을 의미. 문서 d가 나타날 확률은 문서에서의 단어 w1 .. wn가 동시적으로 그리고 독립적으로 일어난 사건을 의미.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;예시로, 다음과 같이 d가 4개, 그 다음 각 문서에서의 단어들 w, 그리고 문서들이 속할 클래스 c가 있습니다.&lt;/p&gt;

&lt;p&gt;P(c_cv)는 문서 cv가 선택될 확률이므로 전체 4개의 문서 중 2개 : 0.5&lt;br /&gt;
P(c_NLP)도 위와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 각 문서에서의 특정 단어가 나타날 확률은 다음의 표와 같을 때, 문서 d5가 주어졌을 때, 어느 클래스에 속할지 구하는 확률은 위에서 구한 공식에 대입하면 다음 그림과 같습니다.&lt;/p&gt;

&lt;p&gt;d5가 cv클래스에 속할 확률 : cv 클래스의 문서가 뽑힐 확률 x P(w|c_cv) of words of w_d5&lt;br /&gt;
d5가 NLP클래스에 속할 확률 : NLP 클래스의 문서가 뽑힐 확률 x P(w|c_NLP) of words of w_d5&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day16-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN - 주재걸 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Deep Learning" /><category term="Math" /></entry><entry><title type="html">Sequential Models - RNN</title><link href="http://localhost:4000/development/boostcamp-week3-day15/" rel="alternate" type="text/html" title="Sequential Models - RNN" /><published>2021-02-05T00:00:00+09:00</published><updated>2021-02-05T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week3-day15</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week3-day15/">&lt;h1 id=&quot;sequential-models---rnn&quot;&gt;Sequential Models - RNN&lt;/h1&gt;

&lt;p&gt;주식, 언어와 같은 Sequential data와 이를 이용한 Sequential model의 정의와 종류에 대해 배웁니다.
그 후 딥러닝에서 sequential data를 다루는 Recurrent Neural Networks 에 대한 정의와 종류에 대해 배웁니다.&lt;/p&gt;

&lt;h2 id=&quot;sequential-model---naive-sequence-model&quot;&gt;Sequential Model - Naive sequence model&lt;/h2&gt;

&lt;p&gt;Sequential Data는 우리의 일상에서 언어나 주식 차트, 영상 등이 해당될 수 있습니다.&lt;/p&gt;

&lt;p&gt;이미지 같은 고정된 차원의 데이터가 아닌 시간에 따라 데이터가 입력이 되는 시계열 데이터에 대해 동작하는 것이 Sequential Model이라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어 현재 무슨 언어가 나올지 예측하는 모델을 생각해 볼 수 있습니다. 시간상으로 전에 나온 단어들로 미래의 단어를 예측한다던지..&lt;/p&gt;

&lt;p&gt;첫번째 단어가 입력이 되면, 두번째 단어는 첫번째 단어를 고려, 세번째 단어는 두번째, 첫번째 단어를 고려.. 반복&lt;/p&gt;

&lt;p&gt;그럼 이 모델은 현재 단어를 예측하기 위해 고려해야 하는 데이터의 개수가 시간이 지남에 따라 계속 증가하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;sequential-model---auto-regressive-model&quot;&gt;Sequential Model - Auto regressive model&lt;/h2&gt;

&lt;p&gt;위의 모델은 현재를 예측할 때 과거의 데이터를 다 고려했다면, 이 방법은 fixed timespan을 두어 과거의 몇개만 고려하는 것입니다.&lt;/p&gt;

&lt;p&gt;예를들어 과거에 대해 5개의 데이터만 보겠다하면 T의 값은 5가 됩니다. &lt;code class=&quot;highlighter-rouge&quot;&gt;x_t = x_1 ~ x_5&lt;/code&gt;까지만 보겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;markov-model&quot;&gt;Markov model&lt;/h2&gt;

&lt;p&gt;이 모델은 현재의 값은 직전 과거의 하나의 데이터에만 dependent 하다는 것을 가정으로 두게됩니다.&lt;/p&gt;

&lt;p&gt;이 모델은 과거의 하나의 데이터만을 이용하기 때문에, 위의 모델에 비해 당연히 많은 과거의 정보를 이용할 수가 없게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;latent-autoregressive-model&quot;&gt;Latent autoregressive model&lt;/h2&gt;

&lt;p&gt;처음 살펴본 두개의 모델의 가장 큰 단점은 과거의 방대한 데이터를 다 고려해야한다는 것입니다.&lt;/p&gt;

&lt;p&gt;이 모델은 모델 중간에 Hidden state가 들어있는 모델입니다. 이 Hidden state가 하는 일은 과거 데이터의 정보를 summary 한다고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;그래서 다음의 타임스텝의 아웃풋은 현재의 인풋과 바로 이전의 Hidden state에 의존하여 작동할 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recurrent-neural-network&quot;&gt;Recurrent Neural Network&lt;/h2&gt;

&lt;p&gt;위에서 설명한 여러가지 모델들을 가장 잘 표현한 것이 Recurrent Neural Network 라고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Recurrent Neural Network는 MLP와 다른 점이 있다면, 자기 자신을 돌아오는 구조가 하나 있다는 것입니다.&lt;/p&gt;

&lt;p&gt;이런 구조에서 타임t에서의 h는 x_t에서만 dependent한 것이 아니라, t-1의 A로 표시된 Sell state에도 dependent 하게 됩니다.&lt;/p&gt;

&lt;p&gt;밑의 그림에서 오른쪽 그림은 왼쪽에서의 그림을 시간순으로 쭉 나열한 것이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;short-term-dependencies&quot;&gt;Short-term dependencies&lt;/h3&gt;

&lt;p&gt;위에서 살펴본 RNN 구조에서의 가장 큰 단점이라고 한다면 Short-term dependencies을 들 수 있습니다.&lt;/p&gt;

&lt;p&gt;과거의 모든 정보들이 다 취합이 되서 요약되고 미래에서 그것을 고려가 되어야하는데, RNN 자체는 하나의 fixed rule로 이 정보들을 계속 취합하기 때문에, 먼 과거에 있던 정보가 미래까지 살아남기 힘든 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;Short-term dependencies 즉, 현재에서 몇개의 전 과거의 데이터는 잘 고려가 되는데, 한 참 멀리 있는 정보를 고려하기 힘든것을 말하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;long-short-term-memory&quot;&gt;Long Short-term Memory&lt;/h3&gt;

&lt;p&gt;기존의 RNN 구조를 살펴보면, x가 A의 구조를 통과해서 h라는 아웃풋을 산출하고 h는 다시 다음번의 A구조로 들어가게 됩니다. 여기서 A구조에서는 weight와 계산이 되고, activation function과 계산이되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;기존의 RNN을 개선한 Long Short-term Memory의 구조는 다음과 같습니다.&lt;/p&gt;

&lt;p&gt;Long Short-term Memory의 각각의 컴포넌트가 어떻게 동작하고 이 구조가 왜 Short-term dependencies를 극복하는지 알아보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Long Short-term Memory의 구조를 세부적으로 묘사한 그림입니다.&lt;/p&gt;

&lt;p&gt;x_t는 인풋으로써 언어 모델이라면 여러개의 단어(5000개의 단어)를 표현한 원-핫 벡터가 될 수도 있고, 워드 임베딩된 벡터로 표현될 수도 있습니다.&lt;/p&gt;

&lt;p&gt;h_t는 hidden state이면서, output이 됩니다.&lt;/p&gt;

&lt;p&gt;previous cell state는 0부터 t-1까지의 정보를 다 취합해서 summarize 해주는 정보가 됩니다.&lt;/p&gt;

&lt;p&gt;previous hidden state는 위쪽으로 previous cell state 들어가기도 하지만 오른쪽으로 여러 연산의 입력으로도 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;전체적으로 보면 들어오는 입력은 3개가 되고 나가는 출력도 3개가 됩니다. 출력 중 2개는 다음번 입력으로 들어가고, 나머지 하나는 실제 아웃풋이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Long Short-term Memory은 총 4개의 gate로 이루어져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그림에서 검은 선이 바로 cell state이고, C_0부터 C_t 까지 들어온 정보를 요약하게 됩니다.&lt;br /&gt;
타임스탬프 t마다 컨베이어 벨트에서 처럼 물건(정보)이 올라오고, 그 정보들을 잘 조작해서, 어떤 것이 유용하고 어떤 것이 유용하지 않은지 정해서 다음번 셀로 이 정보들을 넘겨주는 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;정보들을 잘 조작해서, 어떤 것이 유용하고 어떤 것이 유용하지 않은지 정해서 올리는 역할은 gate가 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Forget gate는 어떤 정보를 버릴지 정하는 역할을 합니다.&lt;br /&gt;
Forget gate에는 현재의 입력 x_t와 이전의 hidden state value인 h_t-1이 들어가서 f_t라는 숫자를 얻어내게 됩니다. 시그모이드를 통과하기 때문에 항상 f_t는 0에서 1사이 값을 갖게 됩니다.&lt;/p&gt;

&lt;p&gt;f_t는 이전 cell state에서 나온 정보들 중에 어떤 것을 버리고, 어떤것을 살릴지 정해주게 됩니다.&lt;/p&gt;

&lt;p&gt;Input gate는 어떤 정보를 cell state에 저장할지(올릴지) 정하는 역할을 합니다.&lt;br /&gt;
현재에 어떤 입력이 들어왔는데, 이것을 무작정 cell state에 올리는 것이 아닌, 이 정보중에 어떤 정보를 올릴지 정하게 됩니다.&lt;/p&gt;

&lt;p&gt;i_t는 어떤 정보를 cell state에 올릴지 또는 저장할지 정하게 됩니다. 이전의 hidden state value와 현재의 입력을 가지고 만들어 지게 됩니다.&lt;/p&gt;

&lt;p&gt;그렇다면 우리가 실제로 올릴 정보를 알아야합니다.&lt;/p&gt;

&lt;p&gt;C_t~가 이 역할을 하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;update gate는 이전까지 summarize 되어있던 cell state와 올릴지 또는 저장할지 정하는 i_t 그리고 C_t~를 가지고 cell state에 update하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;이전 cell state인 C_t-1과 f_t 만큼을 곱해서 C_t-1에서 어떤 정보를 버리게 되고 나오는 값 A, C_t~와 i_t만큼을 곱해서 어느 값을 올릴지를 정해서 만들어진 값 B, 이 두 값을 combine해서 다음 cell state인 C_t으로 업데이트 하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;Out state는 어떤 값을 밖으로 내보낼지 결정하는 역할을 합니다.&lt;/p&gt;

&lt;p&gt;o_t는 어떤 값을 내보낼지 정하는 역할을 하게 되고 이를 thanh를 통과한 C_t인 updated cell state와 Element wise multiplication을 해서 결정적으로 h_t가 만들어 지게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day15-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;RNN - 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /></entry><entry><title type="html">Regularization</title><link href="http://localhost:4000/development/boostcamp-week3-day14/" rel="alternate" type="text/html" title="Regularization" /><published>2021-02-04T00:00:00+09:00</published><updated>2021-02-04T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week3-day14</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week3-day14/">&lt;h1 id=&quot;regularization&quot;&gt;Regularization&lt;/h1&gt;

&lt;p&gt;Regularization은 우리말로 규제라고 하며 쉽게 말해서 이전에 살펴본 최적화 방법에서 우리의 모델을 Generalization하게(학습 데이터에 대한 에러와 테스트 데이터에 대한 에러의 차이가 가능한 작게 되도록)하도록 하는 의미를 갖습니다.&lt;/p&gt;

&lt;p&gt;엄밀히 말하면 우리의 모델이 데이터를 보고 학습을 하게 되는데, 학습에 방해가 되도록 규제를 하게 됩니다.&lt;br /&gt;
학습에 방해가 되도록 규제를 해서 우리가 얻는 이점은 학습 데이터에서만 우리의 모델이 잘 동작하는 것만 아니라 한번도 보지 못한 테스트 데이터에서도 잘 동작하도록 하는 것입니다.&lt;/p&gt;

&lt;p&gt;여러가지 규제 방법들이 있고 아래와 같은 순서로 하나씩 살펴보겠습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Early stopping&lt;/li&gt;
  &lt;li&gt;Parameter norm penalty&lt;/li&gt;
  &lt;li&gt;Data augmentation&lt;/li&gt;
  &lt;li&gt;Noise robustness&lt;/li&gt;
  &lt;li&gt;Label smoothing&lt;/li&gt;
  &lt;li&gt;Dropout&lt;/li&gt;
  &lt;li&gt;Batch normalization&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;early-stopping&quot;&gt;Early stopping&lt;/h2&gt;

&lt;p&gt;일반적으로 학습이 반복되면서 학습 에러는 낮아지지만 그에 반해 학습 데이터 셋에서 분리한 검증 데이터로 검증을 하게 될 때, 검증 에러는 증가하게 됩니다.&lt;/p&gt;

&lt;p&gt;밑의 그림은 말 그대로 학습이 반복됨에 따라 Generalization performance가 낮아지기 전에 또는 Generalization Gap이 커지기 전에 우리의 모델 학습을 멈추는 것을 의미합니다. 하지만 너무 빨리 멈춰 버리면 충분한 학습이 덜 되는 등 문제가 생기게 되므로 상황에 맞게 잘 적용해야 할 것입니다.&lt;/p&gt;

&lt;p&gt;규제를 적용하고자 할 때 테스트 데이터를 써서 Early stopping을 하게 되면 Cheating에 해당되게 되므로 검증 데이터를 따로 사용하고 테스트 데이터는 사용하지 않습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;parameter-norm-penalty&quot;&gt;Parameter Norm Penalty&lt;/h2&gt;

&lt;p&gt;Parameter Norm Penalty는 뉴럴 네트워크 파라미터가 너무 커지지 않게 하는 것을 말합니다.&lt;br /&gt;
네트워크 파라미터 숫자를 다 제곱한 다음 더하면 어떤 값이 나오게 되는데 이 숫자를 같이 줄이는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;이 것에 대해 물리적이거나 해석적인 의미는 뉴럴 네트워크가 만들어 내는 함수의 공간에 부드러움을 더하는 것입니다. 부드러운 함수일 수록 Generalization performance가 높을 것이다 라는 가정을 가지게 됩니다&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;밑에서 알아보게 될 stride와 padding는 고려하지 않고 가장 기본적인 Convolution 연산을 해보면 오른쪽의 Output 필터가 도출되게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;data-augmentation&quot;&gt;Data Augmentation&lt;/h2&gt;

&lt;p&gt;뉴럴넷 혹은 딥러닝에서는 가장 중요한 하나를 꼽자면 역시 데이터 인데요, 데이터의 수가 작은 것 보다 데이터가 무한히 많으면 확실히 학습이 잘 이루어 지게 됩니다.&lt;/p&gt;

&lt;p&gt;왼쪽의 그림 처럼 데이터의 수가 적을 때는 Random Forest, XGBoost 같은 방법론들이 데이터 대비 학습 성능이 더 잘 될 때가 많았습니다.&lt;/p&gt;

&lt;p&gt;하지만 데이터 샘플의 개수가 어느정도 커지게 되면 딥러닝은 우리가 가지고 있는 방대한 데이터를 잘 표현을 하게 되고 이로 인해 성능이 올라가고, 기존에 머신러닝 방법론들은 이 많은 수의 데이터를 표현할 만한 표현력이 부족해지게 됩니다.&lt;/p&gt;

&lt;p&gt;문제는 무엇이냐 하면, 현실에서의 데이터의 수는 한정적이고 해서 Data Augmentation를 통해서 데이터에 조작을 가해 기존에 있던 데이터와 비슷하지만 형태가 조금 다른 데이터를 추가하는 방식을 만들게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위에서 보이는 사진처럼 기존의 이미지에 대한 레이블 값이 바뀌게 하지 않는 선(유지)에서 이미지를 회전시키고, 확대 시키거나 축소 또는 뒤집기(플립), 자르기 등의 행위를 Data Augmentation 이라고 합니다.&lt;/p&gt;

&lt;p&gt;하지만 MNIST 데이터에서 6과 같은 숫자는 상하로 뒤집게 되면 아예 이 데이터에 대한 레이블 값이 바뀌는 것이기에 이런 경우에는 뒤집기 같은 것을 사용할 수 없게 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;noise-robustness&quot;&gt;Noise Robustness&lt;/h2&gt;

&lt;p&gt;Noise Robustness는 노이즈를 추가한 데이터에 대해서도 잘 동작 하도록 모델을 강건하게 만든다는 것에 의미가 있습니다.&lt;/p&gt;

&lt;p&gt;Data Augmentation과 차이점이라면 입력 데이터에 대해서만 노이즈를 추가하는 것이 아닌, 모델의 weight에 대해서도 노이즈를 추가해 줄 수 있습니다. 이렇게 되면 모델이 학습할 때 조금 더 성능이 더 잘 나온다는 실험적 결과가 있다고 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;label-smoothing&quot;&gt;Label Smoothing&lt;/h2&gt;

&lt;p&gt;Label Smoothing도 Data Augmentation과 비슷한 의미를 갖는데, 차이점은 분류 문제라면 학습 데이터에서 랜덤하게 두 개의 이미지를 가져와 섞어주는 것입니다.&lt;/p&gt;

&lt;p&gt;이것이 어떤 효과가 있냐 하면, 분류 문제에서의 이미지 공간 속에서 여러 개의 클래스들을 잘 구분할 수 있는 Decision Boundary를 찾고 싶을 것인데, 이 Decision Boundary를 부드럽게 만들어주는 효과를 가지고 있다고 합니다.
&lt;img src=&quot;/assets/images/aitech_day14-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Mix up 같은 경우는 밑에서 보이는 것 처럼 이미지를 서로 50:50으로 섞습니다. 그리고 레이블 값도 각각 0.5씩 주게 됩니다.&lt;/p&gt;

&lt;p&gt;Cut out은 이미지에서 일정 영역을 빼버리게 됩니다.&lt;/p&gt;

&lt;p&gt;Cut mix는 이미지를 섞는데, 블렌딩하게 섞는 것이 아닌 일정 영역을 잘라 다른 이미지로 대체해버리는 방법을 말합니다. 그림에서 보이듯이 레이블 값은 0.6, 0.4씩 주어지게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;dropout&quot;&gt;Dropout&lt;/h2&gt;

&lt;p&gt;드롭아웃은 사진에서 보이는 것 처럼 뉴럴 넷에서의 특정 웨이트를 0으로 만듦으로써 뉴런을 비활성화 시키는 것을 말합니다. 드롭아웃 p = 0.5라는 말은 50%의 뉴런을 비활성화 시킨다라고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;이것을 해서 얻는 이점은 각각의 뉴런들이 좀 더 Robust한 feature를 잡을 수 있다라고 해석을 한다고 합니다. 그래서 모델의 Generalization performance가 올라간다고 실험적으로 잘 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;batch-normalization&quot;&gt;Batch Normalization&lt;/h2&gt;

&lt;p&gt;Batch Normalization는 이 기법을 적용하고자 하는 레이어의 값들을 통계적으로 정규화 시킨다라고 말할 수 있습니다.&lt;/p&gt;

&lt;p&gt;예를 들어 1000개의 파라미터 값이 있는 히든 레이어가 있다고 하면, 1000개의 파라미터 각각의 값들에 대해 zero mean and unit variance하게 만들어 주는 것입니다.&lt;/p&gt;

&lt;p&gt;unit variance은 데이터들에 대해 분산이 1이고 평균이 0인 값들의 분포로 만들어 주는 것을 말합니다.&lt;/p&gt;

&lt;p&gt;신경망에서 해당층에는 파라미터의 여러 값들이 있는데, 그 수치의 크기가 달라서 큰 수의 값에 신경망이 지나치게 영향을 받는 것을 방지하기 위해서 이 기법을 적용한다고 생각해 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;방법은 전체 데이터에 대해 평균과 분산을 구하고, 각각의 데이터에 대해서 평균을 뺀 값을 표준편차로 나눠주게 되면 Standarization이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;그렇다면, 전체 데이터에서 평균에 가까운 값이나 평균의 값을 가진 파라미터는 0또는 0 근처의 값이 되고, 다른 값들은 평균을 빼준 값에서 표준편차로 나누어준 거리 만큼 평균에서 떨어진 값으로 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day14-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Optimization - Convolution - 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">학습 데이터에 대한 에러와 테스트 데이터에 대한 에러의 차이가 가능한 작게 되도록 하는 여러가지 기법을 알아봅니다.</summary></entry><entry><title type="html">CNN - Convolution은 무엇인가?</title><link href="http://localhost:4000/development/boostcamp-week3-day13/" rel="alternate" type="text/html" title="CNN - Convolution은 무엇인가?" /><published>2021-02-03T00:00:00+09:00</published><updated>2021-02-03T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week3-day13</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week3-day13/">&lt;h1 id=&quot;cnnconvolutional-neural-network&quot;&gt;CNN(Convolutional Neural Network)&lt;/h1&gt;

&lt;p&gt;CNN(Convolutional Neural Network)에서 가장 중요한 연산은 Convolution 입니다.&lt;br /&gt;
Convolution의 정의, convolution 연산 방법과 기능에 대해 배웁니다.&lt;br /&gt;
그리고 Convolution, 입력을 축소하는 Pooling layer, 모든 노드를 연결하여 최종적인 결과를 만드는 Fully connected layer로 구성되는 기본적인 CNN(Convolutional Neural Network) 구조에 대해 배웁니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;CNN consists of convolution layer, pooling layer, and fully connected layer.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;가장 기본적이고 고전적인 CNN은 feature 값을 얻어내기 위해 합성곱 연산을 하는 &lt;strong&gt;convolution layer&lt;/strong&gt;, Convolution을 거쳐서 나온 activation maps이 있을 때, 이를 이루는 convolution layer을 resizing하여 새로운 layer를 얻는 &lt;strong&gt;pooling layer&lt;/strong&gt; 그리고 우리가 최종적으로 얻고자 하는 값들을 만들어주는 &lt;strong&gt;fully connected layer&lt;/strong&gt;로 구성됩니다.&lt;/p&gt;

&lt;p&gt;이미지를 예로 들면, Convolution and pooling layer는 이미지에서 어떤 유용한 정보를 뽑아내는 &lt;strong&gt;feature extraction&lt;/strong&gt;을 담당하고 fully connected layer는 &lt;strong&gt;decision making&lt;/strong&gt;; 예를 들어 분류를 하거나 회귀를 해서 우리가 원하는 출력 값을 얻도록 하게 합니다.&lt;/p&gt;

&lt;p&gt;최근 들어서는 뒷단에 있는 fully connected layer가 최소화 되고, 점점 없어지는 추세라고 합니다.&lt;br /&gt;
이것은 패러매터의 수와 연관이 있는데요, 패러미터의 수가 늘어 나면 늘어날 수록 학습이 어렵고, Generalization performance가 떨어진다고 알려져 있습니다.&lt;/p&gt;

&lt;p&gt;그래서 CNN의 응용 모델에서는 이런 패러매터의 수를 줄이면서 굉장히 Deep한 모델을 만들기 위해 여러 기법이 동원되고 적용되고 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;convolution&quot;&gt;Convolution&lt;/h2&gt;

&lt;p&gt;Convolution 연산을 밑의 그림을 통해 봐보겠습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;밑에서 알아보게 될 stride와 padding는 고려하지 않고 가장 기본적인 Convolution 연산을 해보면 오른쪽의 Output 필터가 도출되게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오른쪽에서 빨간 네모인 값, 즉 하나의 Output 값은 Convolution 필터를 적용하고자 하는 이미지에 찍어 도출한다고 생각할 수 있습니다.&lt;/p&gt;

&lt;p&gt;위의 그림에서 보는 것 처럼, 필터를 좌측 상단 이미지 상에 위치시키고, 각 그리드 위치에 맞는 값들 끼리 서로 곱셈을 한 후 전부 더하면 O_11의 output 값이 됩니다.&lt;/p&gt;

&lt;p&gt;추가로 마지막에 바이어스 텀을 더해줄 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전과 마찬가지로 O_12, O_13 값도 이미지 상에서 필터를 한 칸씩 옆으로 밀어 똑같이 계산하면 도출할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;2D convolution in action&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;이미지 상에서 convolution operation을 한다는 것은 어떤 의미가 있을까요?&lt;/p&gt;

&lt;p&gt;우리가 적용하고자 하는 필터에 따라 같은 이미지에 대해서 convolution output이 밑에서 보이는 것 처럼 여러가지 타입의 이미지로 변환이 되게 됩니다.&lt;/p&gt;

&lt;p&gt;예를 들어서, 1/9 값을 가진 3x3 필터를 사용한다면, 이미지 상에서 필터가 적용될 때 3x3 픽셀의 이미지 값의 평균이 계산되고 이는 output 값에서 한 픽셀의 값이 됩니다. 그래서 이미지의 특정 영역에 대해 픽셀 값을 다 합쳐서 평균을 내므로 마치 밑에서 보이는 것 처럼 예를 들면 Blur화 된 이미지가 만들어지게 되는 것입니다.&lt;/p&gt;

&lt;p&gt;마찬가지로 다른 타입의 필터를 적용하면 같은 이미지에 대해 강조되거나 외곽이 강조된 이미지를 얻을 수 있을 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;rgb-image-convolution&quot;&gt;RGB Image Convolution&lt;/h2&gt;

&lt;p&gt;일반적으로 CNN에서는 RGB 이미지를 많이 다루게 됩니다.&lt;/p&gt;

&lt;p&gt;대체로, 이미지는 가로와 세로 길이 그리고, RGB 값을 의미하는 3개의 채널로 구성이 됩니다.&lt;/p&gt;

&lt;p&gt;RGB 이미지에 대해 Convolution을 하게 되면 이미지의 채널과 적용하고자 하는 필터의 채널 수를 맞춰서 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;그래서 Output은 가로와 세로 모두 32-5+1인 크기가 되고 Depth즉 채널의 수는 1이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;하나의 이미지에 대해 하나의 필터를 적용하게 되면 깊이 즉, 채널이 1인 feature 맵이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;feature맵의 채널 수를 늘리고 싶다면, 하나의 이미지에 대해 여러개의 서로 다른 filter를 사용해서 연산을 하게 되면 feature맵의 채널 수가 filter개수 만큼 늘어나게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Convolution 연산을 여러 번 하게 되면 아래 그림처럼 도식화 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;stride는 1, padding은 0으로 고려하고 도식화 된 이미지 입니다.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;32x32x3의 이미지에 대해 5x5x3의 필터가 적용되게 되는데, 필터의 개수가 4개 이므로 결과물을 28x28x4가 됩니다. (적용하는 필터의 개수 = 출력의 채널 수) 그리고 한 번 Convolution이 거치게 되면 생성된 feature맵의 각각의 Element들에 대해 Non-linear activation function을 적용하게 됩니다.&lt;/p&gt;

&lt;p&gt;ReLU를 적용시키게 되면, 0보다 작은 Element 값들은 0 이되고, 0보다 큰 값들은 그대로 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;그리고 다시 이 feature맵에 대해 5x5x4의 필터 10개를 적용하게 되면 그림과 같이 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;나중에 보게 되겠지만 이 그림에서 첫번 째 Convolution에서 적용된 패러매터 들의 개수는 5x5x3x4가 됩니다.&lt;br /&gt;
4개의 필터가 필요하고, 각 필터의 사이즈는 5x5x3가 되기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;stride&quot;&gt;Stride&lt;/h2&gt;

&lt;p&gt;Stride라는 말은 쉽게 생각해 넓게 걷는다 라는 의미가 됩니다.&lt;/p&gt;

&lt;p&gt;Convolution에서 Stride가 1이란 말은 이미지 상에서 매 픽셀마다 필터를 한 번에 한 픽셀 씩 옮겨 적용한다고 생각하면 이해가 쉽습니다.&lt;/p&gt;

&lt;p&gt;위에서 보았던 Convolution 연산들은 이미지에 대해 필터가 한 번에 한 칸씩 이동하며 적용이 되었습니다. 그래서 Stride가 1이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;왼쪽 그림에서 Input에 대해 크기가 3인 필터를 적용하고 Stride가 1이라면 Output은 5가 됩니다. 한 번에 한 칸씩 가므로.&lt;/p&gt;

&lt;p&gt;Stride가 2인 경우에는 처음에 필터가 적용되고 나서 Stride의 수 만큼 옆으로 필터가 이동하게 됩니다. 따라서 Output은 3개가 나오게 됩니다.&lt;/p&gt;

&lt;p&gt;위에서는 1차원을 예로 들었지만 2차원이 되면, Stride도 (1, 1), (2, 3) 이렇게 표현 될 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;padding&quot;&gt;Padding&lt;/h2&gt;

&lt;p&gt;위에서 보았듯이 이미지에 대해 필터를 3x3을 적용하고 나온 출력은 원래 이미지의 크기와 같지 않고 축소 되었습니다. 왜냐하면 boundary 정보가 버려지기 때문입니다. 필터가 이미지 밖으로 삐져나와서는 계산이 되지 않는 것을 말한 것입니다. 그래서 Padding은 가장자리에도 필터를 적용하고자 해서, 이미지의 가장자리에 0같은 값으로 채워주는 것을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;5 크기의 Input이 있을 때 3 크기의 필터가 있으면 출력은 3이 됩니다.&lt;/p&gt;

&lt;p&gt;출력을 원래 Input 크기만큼의 5로 만들어 주려면 가장 자리에 0을 덧댐으로서 즉 0 Padding을 함으로써 5로 만들어 줄 수 있게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Stride&amp;amp;Padding&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;다음은 2차원 상에서 Stride 와 Padding을 다르게 하여 Convolution 연산을 도식화 한 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;convolution-arithmetic&quot;&gt;Convolution Arithmetic&lt;/h2&gt;

&lt;p&gt;밑의 그림을 보고 Convolution 연산이 될 때의 parameter 수를 계산해 봅시다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day13-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;논문을 보거나 대부분의 자료에서 이렇게 도식화 된 그림을 많이 볼 수 있고, 도형 옆에는 각각 수들이 적혀 있습니다.&lt;/p&gt;

&lt;p&gt;H: 세로 길이를 의미합니다.&lt;br /&gt;
W: 가로 길이를 의미합니다.&lt;br /&gt;
C: 채널 수, 깊이를 의미합니다. 보통 이미지라면 R, G, B의 3개의 값이고 투명도 까지 더해지면 4개의 값이 됩니다.&lt;/p&gt;

&lt;p&gt;파란색 네모 박스는 왼쪽 큰 상자에 대해 적용할 필터가 됩니다.&lt;br /&gt;
3x3 크기의 필터를 적용하겠다는 것이고, 당연히 큰 상자의 채널 수와 필터의 채널 수는 일치해야 합니다.&lt;/p&gt;

&lt;p&gt;그렇다면 필터의 패러미터 수는 3x3x128이 됩니다.&lt;/p&gt;

&lt;p&gt;오른쪽의 출력의 채널 수는 64개가 되는데 이는, 오른쪽의 박스가 파란색 네모 필터를 64개 적용해서 얻어진 결과물이 됩니다. 왜냐하면 파란색 네모인 필터 하나 당 채널이 1인 feature map을 만들기 때문입니다.&lt;/p&gt;

&lt;p&gt;그래서 총 패러미터의 수는 3x3x128x64가 됩니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;CNN - Convolution - 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">Convolution의 정의, convolution 연산 방법과 기능에 대해 배웁니다.</summary></entry><entry><title type="html">Optimization in Deep Learning</title><link href="http://localhost:4000/development/boostcamp-week3-day12/" rel="alternate" type="text/html" title="Optimization in Deep Learning" /><published>2021-02-02T00:00:00+09:00</published><updated>2021-02-01T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week3-day12</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week3-day12/">&lt;h1 id=&quot;optimization-in-deep-learning&quot;&gt;Optimization in Deep Learning&lt;/h1&gt;

&lt;p&gt;최적화와 관련된 주요한 용어와 내용(Generalization, Overfitting, Cross-validation) 그리고 기존 SGD(Stochastic gradient descent)를 넘어서 최적화(학습)가 더 잘될 수 있도록 하는 다양한 기법들에 대해 알아봅니다.&lt;/p&gt;

&lt;h2 id=&quot;important-concepts-in-optimization&quot;&gt;Important Concepts in Optimization&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Generalization&lt;/li&gt;
  &lt;li&gt;Under-fitting vs. over-fitting&lt;/li&gt;
  &lt;li&gt;Cross validation&lt;/li&gt;
  &lt;li&gt;Bias-variance&lt;/li&gt;
  &lt;li&gt;Bootstrapping&lt;/li&gt;
  &lt;li&gt;Bagging and boosting&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;generalization&quot;&gt;Generalization&lt;/h2&gt;

&lt;p&gt;일반화는 학습 데이터로 학습이 되어진 모델이 얼마나 테스트 데이터에서 잘 동작하는지(문제에 따라 에러를 최소화)를 나타내는 개념입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;위의 그림처럼 학습 데이터를 반복적으로 학습함에 따라 모델은 Training Error는 줄어들게 됩니다.&lt;br /&gt;
그렇다고 Training Error가 최소값이 되었다고 해서 우리의 모델이 최적화 되었다고 할 수는 없습니다.&lt;/p&gt;

&lt;p&gt;일반적으로 Training Error가 줄어들지만, 어느정도 반복 학습이 이루어지고 시간이 지나면 학습하지 않은(한번도 보지 않은) 테스트 데이터에 대해서는 Test Error가 올라가게 됩니다.&lt;/p&gt;

&lt;p&gt;Generalization Gap은 Training Error와 Test Error 사이의 차이를 의미합니다.&lt;/p&gt;

&lt;p&gt;그래서 Training Error와 Test Error의 차이 즉, Generalization Gap이 작을 때 우리는 Generalization Performance가 좋다고 말할 수 있고, Generalization Gap이 클 때 Generalization Performance가 좋지 않다고 말 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;주의할 점은 Test Error가 낮다고 해서 Generalization Performance 좋은 것이 아닙니다.&lt;br /&gt;
Generalization Performance는 단지 두 Error의 차이에 따른 성능만을 의미하는 것이기 때문입니다.&lt;/p&gt;

&lt;h2 id=&quot;underfitting-vs-overfitting&quot;&gt;Underfitting vs. Overfitting&lt;/h2&gt;

&lt;p&gt;&lt;strong&gt;Overfitting&lt;/strong&gt;은 학습데이터에 대해서는 잘 동작 하지만 테스트 데이터에 대해서는 잘 동작하지 않는 현상을 의미합니다. 이는 모델이 학습 데이터에 대해 너무 과적합하게 학습된 나머지, 한 번도 보지 못한 테스트 데이터에 대해서는 잘 동작하지 않는 것을 의미하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Underfitting&lt;/strong&gt;은 네트워크가 너무 간단하거나 학습을 너무 조금 시켜서 학습 데이터 마저도 잘 맞추지 못하는 현상을 의미하게 됩니다.&lt;/p&gt;

&lt;p&gt;밑의 그림에서 볼 수 있듯이 학습데이터에 대해 너무 과적합하지 않으면서도 간단하거나 부족하지 않은 중간의 모델이 Balance하다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;하지만 문제에 따라서는 오른쪽 그림과 같이 파란 선들이 Target일 수도 있어서 이 개념들은 컨셉적인 이야기라고 보면 될 것 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;cross-validation&quot;&gt;Cross-validation&lt;/h2&gt;

&lt;p&gt;전체 데이터를 크게 학습 데이터와 테스트 데이터로 나누는데, 학습 데이터에서 일정 부분을 검증을 위해 때어내서 만든 데이터를 &lt;strong&gt;Validation Data&lt;/strong&gt; 라고 합니다.&lt;/p&gt;

&lt;p&gt;Validation Data를 만들어 사용하는 이유는 학습 데이터로 학습을 시킨 모델이 &lt;strong&gt;학습에 사용되지 않은&lt;/strong&gt; Validation Data 기준으로 얼마나 &lt;strong&gt;잘 동작하는지&lt;/strong&gt; 알아보기 위함입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;K-fold Validation이라고도 불리는 Cross validation은 학습 데이터를 k개의 fold로 나누고, 1개의 fold는 검증에 사용할 validation data로, 나머지 k-1개의 fold는 학습 데이터로 사용하는 것입니다.&lt;br /&gt;
이를 fold 마다 한번 씩 번갈아 가며 k번 수행하면 총 5개의 서로 다른 validation 값이 나오게 되고, 이 값들을 평균낸 값을 사용하게 됩니다.&lt;/p&gt;

&lt;p&gt;모델을 학습 시키는데에 있어서 중요한 하이퍼 패러미터(Learning Late, 손실함수, 네트워크의 크기 등)를 정하기 위해 먼저 교차 검증을 수행하고 하이퍼 패러미터가 교정이 되면, 검증 데이터를 학습 데이터에 포함시켜 전체 학습 데이터에 대해 모델을 학습합니다. 이 때, 테스트 데이터를 사용해서 교차검증을 한다거나 하이퍼 패러매터를 설정하는 것 등… 무슨 이유에서건 테스트 데이터를 학습에 사용되서는 안됩니다.&lt;/p&gt;

&lt;h2 id=&quot;bias-and-variance&quot;&gt;Bias and Variance&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Low Variance는 비슷한 입력을 넣었을 때, 얼마나 일관된 출력을 내는지를 의미합니다.
이는 주로 모델이 간단할 때 나타나는 현상입니다.&lt;/li&gt;
  &lt;li&gt;High Variance는 비슷한 입력을 넣었을 때, 출력이 많이 달라지게 되는 것을 의미합니다.
이렇게 되면 모델이 Overfitting 될 가능성이 커지게 됩니다.&lt;/li&gt;
  &lt;li&gt;Low Bias는 전체적인 출력 값이 Target 또는 Mean과 가깝다는 것을 의미합니다.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;High Bias는 전체적인 출력 값이 Target 또는 Mean과 다소 떨어져 있다는 것을 의미합니다.&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;bootstrapping&quot;&gt;Bootstrapping&lt;/h2&gt;

&lt;p&gt;고정된 수의 학습데이터에 대해 랜덤하게 샘플링 된 데이터로 여러 모델을 만들어 어떤 목적을 수행하겠다는 방법론을 말합니다.&lt;/p&gt;

&lt;p&gt;예를 들어 전체 데이터의 수가 100개이면 그 중 랜덤하게 80개를 뽑아 모델 1을 학습시키고, 다시 랜덤하게 80개를 뽑아 모델 2를 학습시키고… 이렇게 수행하게 됩니다.&lt;/p&gt;

&lt;p&gt;이렇게 되면 하나의 입력에 대해서 여러개의 모델이 같은 값을 예측할 수도 있지만 서로 다른 값을 예측할 수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래서 입력 값들에 대해 이 모델들의 출력 값이 얼마나 일치하는 지를 보고 전체적인 모델의 Uncertainty를 예측하고자 할 때 사용할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;bagging-vs-boosting&quot;&gt;Bagging vs Boosting&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Bagging&lt;/strong&gt; : 위에서 다룬 Bootstrapping 방법을 쓰는 데, 전체 데이터에서 일부를 랜덤 샘플링 해서 여러 개의 모델을 만들고, 그 모델들의 아웃풋을 가지고 평균을 내는 것을 의미하게 됩니다. 이런 것을 일반적으로 앙상블이라고도 부릅니다.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Boosting&lt;/strong&gt; : 전체적인 프로세스가 Sequential 하게 전체 학습 데이터에 대해 모델을 만들고 그 중 학습이 잘 안된 데이터에 대해 또 다른 모델을 만들어 이 학습이 잘 안된 데이터에 대해 학습이 잘 되도록 모델을 만드는 것을 반복합니다. 그리고 이렇게 만든 모델들을 Sequential하게 합쳐 하나의 모델을 만듭니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-methods&quot;&gt;Gradient Descent Methods&lt;/h2&gt;

&lt;p&gt;Gradient Descent 분류해보면 크게 3가지로 분류해 볼 수 있습니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Stochastic gradient descent : 엄밀하게 얘기해서 전체 학습 데이터에서 한번에 한개의 샘플 데이터에 대해서 역전파를 통해 그레디언트를 구하고, 값을 업데이트하고, 다시 한번에 한개의 샘플 데이터에 대해 반복하는 것을 말합니다.&lt;/li&gt;
  &lt;li&gt;Mini-batch gradient descent : 배치 사이즈인 256개나 128개 등의 학습 데이터의 서브셋에 대해 계산된 그레디언트를 업데이트 하는 방법을 말합니다.&lt;/li&gt;
  &lt;li&gt;Batch gradient descent : 전체 데이터에 대해 계산된 그레디언트를 사용해 패러매터를 업데이트 하게 되는 것을 말합니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;대부분의 딥러닝 문제에서는 Mini-batch gradient descent가 사용되어지고 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;batch-size-matters&quot;&gt;Batch-size Matters&lt;/h2&gt;

&lt;p&gt;많은 딥러닝 문제에서 Batch-size는 아주 중요한 요소입니다.&lt;br /&gt;
Batch-size에 따라 우리가 추구하고자 하는 모델이 이상적으로 가는지, 잘 학습이 되어지는에 대해 결정적일 수 있기 때문입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;실험적으로 발견이 되어진 내용에 의하면 큰 Batch-size를 사용하여 모델을 학습하게 되면, 학습과 테스트 함수가 sharp minimizers에 도달할 가능성이 크다고 합니다. 반면에 작은 Batch-size를 사용하게 되면 flat minimizers에 도달할 가능성이 크다고 합니다.&lt;/p&gt;

&lt;p&gt;여기서 말하고자 하는 내용은 Batch-size를 작게쓰는 것이 일반적으로 성능이 좋다라는 것을 실험적으로 말하는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리의 목적은 데이터가 Testing function에서 잘 동작하는 것을 찾고 싶습니다.&lt;/p&gt;

&lt;p&gt;Flat minimum에서 Training Function 값이 조금 멀어져도, Testing Function에서도 적당히 비슷한 값이 나오는 것을 볼 수 있습니다. 이 말은 학습 데이터에서 잘 동작하는 것이 테스트 데이터에서도 잘 동작하는 즉, 앞에서 살펴본 Generalization performance가 높다고 할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Sharp minimum 에서는 Training Function에서 Local minimum에 도달했어도 Testing Function에서는 약간만 멀어져 있어도 높은 값이 나오는 것을 볼 수 있습니다. 이 말은 학습 데이터에서 잘 동작하는 것이 테스트 데이터에서는 잘 동작하지 않는 또는 예측하지 않는 즉, Generalization performance가 낮다고 할 수 있습니다.&lt;/p&gt;

&lt;h2 id=&quot;gradient-descent-methods-1&quot;&gt;Gradient Descent Methods&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Stochastic gradient descent&lt;/li&gt;
  &lt;li&gt;Momentum&lt;/li&gt;
  &lt;li&gt;Nesterov accelerated gradient&lt;/li&gt;
  &lt;li&gt;Adagrad&lt;/li&gt;
  &lt;li&gt;Adadelta&lt;/li&gt;
  &lt;li&gt;RMSprop&lt;/li&gt;
  &lt;li&gt;Adam&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;gradient-descent&quot;&gt;Gradient Descent&lt;/h3&gt;

&lt;p&gt;가장 일반적인 Gradient Descent 방법입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;W는 업데이트 될 패러미터 백터, t는 타임스탬프, g는 자동미분으로 얻어진 그레디언트를 의미합니다.&lt;/p&gt;

&lt;p&gt;그레디언트 값을 에타라고도 부르는 Learning rate 값과 곱해서 W와 빼주고 그 값을 새로 업데이트 하게 되는 방법입니다.&lt;/p&gt;

&lt;p&gt;이 방법의 가장 큰 문제는 스텝 사이즈인 Learning rate를 정하는 것이 너무 어렵다는 점입니다.&lt;br /&gt;
Learning rate가 너무 크게 되면 학습이 잘 안되게 되고, 이 것이 너무 작으면 아무리 학습을 시켜도 학습이 안되게 될 것입니다.&lt;/p&gt;

&lt;p&gt;그래서 이 Learning rate를 적절히 잡아주는게 중요합니다.&lt;/p&gt;

&lt;h3 id=&quot;momentum&quot;&gt;Momentum&lt;/h3&gt;

&lt;p&gt;관성이라고도 하는 이 방법은 쉽게 말해 한 번 그레디언트 방향이 a방향으로 흘렀다면, 다음번에 업데이트에서 계산된 그레이언트가 조금 다른 방향으로 흘러도 전에 흐르던 방향인 a방향의 정보를 조금 추가해서 그 방향을 보정하거나 이어가자는 것을 의미합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;베타상수가 모멘텀이 되고 a_t+1이 accumulation이 됩니다.&lt;br /&gt;
즉 한 번 계산되고 나서 g_t가 버려지게 되는 것이 아니고, 이 값이 베타 상수인 모멘텀과 같이 계산되어 accumuaation에 들어가게 되고 현재 뿐만 아니라, 다음 번의 경사하강 업데이트 계산에 a_t로서 계산이 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;미니 배치 연산에서 에를 들면 전의 배치에서는 경사가 이쪽 방향으로 흘렀지만 다음 번 배치에서는 다른 방향으로 흐를 수 있고 이 때, 전에 계산된 방향으로 조금 보정해서 또는 관성을 줘서 흘러가자라는 것을 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;그래서 그레디언트가 아주 많이 다 방향으로 왔다갔다 해도 어느정도 잘 학습이 되게 만들어 주는 효과가 있다고 합니다.&lt;/p&gt;

&lt;h3 id=&quot;nesterov-accelerated-gradient&quot;&gt;Nesterov Accelerated Gradient&lt;/h3&gt;

&lt;p&gt;NAG라고 불리는 이 경사하강 방법도 위에서 살펴본 Momentum과 비슷한 모양을 띄고 있습니다.&lt;br /&gt;
컨셉적으로 a라는 Accumulate gradient가 Learning rate와 곱해져서 Gradient Descent를 하게 되는 것인데, 위에 것과 조금 다른 점이라면 위에서 살펴본 g_t gradient를 계산 할 때 Lookahead gradient를 계산하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Lookahead gradient는 a라고 불리는 현재 gradient 정보가 있으면, 그 방향으로 한 번 가보고, 간 곳에서 다시 gradient를 계산에 accumulate 시켜주게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Nesterov Accelerated Gradient 방법은 Momentum에 비해 local minima에 조금 더 빨리 converge 할 수 있다고 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;adagrad&quot;&gt;Adagrad&lt;/h3&gt;

&lt;p&gt;이 방법은 뉴럴 네트워크가 학습하면서 시간적으로 얼마나 패러미터가 변해왔는지 안 변해 왔는지를 보게 되고, 값이 많이 변한 패러미터들에 대해서는 다음번에 더 적게 변화 시키고, 반대로 값이 적게 변한 패러미터들에 대해서는 다음번에 더 크게 변화 시키게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-14.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;G_t는 학습하면서 지금까지 얼머나 gradient가 변했는지를 제곱해서 더한 값입니다.&lt;/p&gt;

&lt;p&gt;이 값이 커진다는 것은 그 해당하는 패러미터들이 많이 변했다는 것을 의미하고, 이 값을 역수를 취해 넣었기 때문에, 다음번의 경사 하강에서는 그 해당하는 패러미터들이 보다 적은 배율로 업데이트가 이루어 집니다.&lt;/p&gt;

&lt;p&gt;위의 룰을 적용해, G_t값이 작을 때는 다음번에 좀 더 많이 패러미터들을 변화시키겠다는 말이 됩니다.&lt;/p&gt;

&lt;p&gt;입실론 상수는 learning rate가 0으로 나눠지는 것을 방지하기 위한 (forNumerical stability) 값이 됩니다.&lt;/p&gt;

&lt;p&gt;Adagrad에서 식에서도 유추해볼 수 있는 가장 큰 문제라면, G_t값이 무한대로 계속 커지게 된다면 분모가 무한대이므로 W_t의 왼쪽에서 계산된 텀인 gradient 값은 무한대로 작아지고 결국, W의 업데이트가 이루어지지 않을 것입니다. 그래서 학습이 진행되면서 뒤로 가면 갈 수록 학습이 멈춰지는 현상이 생긴다는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;adadelta&quot;&gt;Adadelta&lt;/h3&gt;

&lt;p&gt;이 방법은 위에서 살펴본 Adagrad에서 G_t 값이 무한대로 커지는 현상을 방지하는 것을 추가한 방법이라고 보면 됩니다.&lt;/p&gt;

&lt;p&gt;Adadelta는 이전의 모든 그래디언트를 누적하는 대신 그래디언트 업데이트의 이동 창을 기반으로 학습률을 조정하는 Adagrad 보다 강력한 확장론적 방법이라고 볼 수 있습니다.&lt;/p&gt;

&lt;p&gt;이렇게 하면 Adadelta는 많은 업데이트가 수행 된 경우에도 계속해서 학습할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-15.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;EMA : Exponential Moving Average&lt;/p&gt;

&lt;p&gt;H_t는 내가 실제로 업데이트 하고하자는 패러미터의 값을 미분하여 제곱한 값이 되고 이를 H_t-1의 값과 감마 상수를 잘 조합한 값이 됩니다.&lt;/p&gt;

&lt;p&gt;이를 위에서 본 G_t값에 대한 역수로 넣어주게 됨으로써 학습이 진행됨에 따라 뒤로가도 학습이 계속 진행되도록 할 수 있습니다.&lt;/p&gt;

&lt;h3 id=&quot;rmsprop&quot;&gt;RMSprop&lt;/h3&gt;

&lt;p&gt;Adadelta에서 살펴본 H_t값과 입실론의 루트 값 대신에 Learning rate텀이 대신 들어간 방법이 됩니다.&lt;/p&gt;

&lt;p&gt;위에서 처럼 G_t를 그냥 쓰는것이 아닌, EMA의 값을 취해서 분모로 취해주게 됩니다.&lt;br /&gt;
분자에는 에타라는 Learning rate가 들어가게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-16.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;adam&quot;&gt;Adam&lt;/h3&gt;

&lt;p&gt;일반적으로 Gradient descent 최적화 방법론 중 가장 잘 되고 무난하게 사용되는 것이 이 방법입니다.&lt;/p&gt;

&lt;p&gt;Gradient squares를 Exponential Moving average로 가져감과 동시에 앞에서 보았던 모멘텀을 같이 활용하는 방법이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day12-17.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그레디언트의 크기가 바뀜에 따라 혹은 그레디언트 제곱의 크기에 따라서 적응적으로 Learning rate를 바꾸는 것과 이전에 계산된 모멘텀을 현재 그레디언트 값과 조합해 새로운 모멘텀을 조합하는 방법을 말하고 있습니다.&lt;/p&gt;

&lt;p&gt;즉, Adam은 momentum과 adaptive Learning rate approach를 조합한 방법이 됩니다.&lt;/p&gt;

&lt;p&gt;Hyperparameter는 베타1(모멘텀을 얼마나 유지시킬지), 베타2(Gradient square에 대한 EMA 정보), 에타(Learning rate), 입실론(for numerical stability)가 있습니다.&lt;/p&gt;

&lt;p&gt;m_t옆에 root(1-b_2) / 1-beta_1 라는 것이 있는데, 이 전체 방법론이 Unbiased estimater가 되기 위해서 수학적으로 정의된 텀입니다.&lt;/p&gt;

&lt;p&gt;딥러닝 프레임워크에서 입실론 패러미터가 기본적으로 약 10의 -7 제곱의 값으로 되어 있는데, 이 값을 잘 조정해주는 것이 Practical한 상황에서는 중요하다고 합니다.&lt;/p&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Optimization in Deep Learning - 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">최적화와 관련된 주요한 용어와 내용들을 알아봅니다.</summary></entry><entry><title type="html">베이즈 정리 / 딥러닝 입문 / MLP</title><link href="http://localhost:4000/development/boostcamp-week3-day11/" rel="alternate" type="text/html" title="베이즈 정리 / 딥러닝 입문 / MLP" /><published>2021-02-01T00:00:00+09:00</published><updated>2021-02-01T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week3-day11</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week3-day11/">&lt;h1 id=&quot;베이즈-정리--딥러닝-입문-mlp&quot;&gt;베이즈 정리 / 딥러닝 입문/ MLP&lt;/h1&gt;

&lt;p&gt;베이즈 정리는 데이터가 새로 추가되었을 때 정보를 업데이트하는 방식에 대한 기반이 되므로 오늘날 머신러닝에 사용되는 예측모형의 방법론으로 굉장히 많이 사용되는 개념입니다.&lt;/p&gt;

&lt;h2 id=&quot;베이즈-정리&quot;&gt;베이즈 정리&lt;/h2&gt;

&lt;p&gt;베이즈 통계학을 이해하기 위해선 조건부확률의 개념을 이해해야 합니다.&lt;/p&gt;

&lt;p&gt;조건부확률 P(A∣B)는 사건B가 일어난 상황에서의 사건A가 발생활 확률을 말합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;베이즈 정리는 조건부확률을 이용하여 정보를 갱신하는 방법을 알려줍니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;베이즈-정리-예제&quot;&gt;베이즈 정리: 예제&lt;/h3&gt;

&lt;p&gt;COVID-99 의 발병률이 10% 로 알려져있다. COVID-99 에 실제로 걸렸을 때 검진될 확률은 99%, 실제로 걸리지 않았을 때 오검진될 확률이 1% 라고 할 때, 어떤 사람이 질병에 걸렸다고 검진결과가 나왔을 때 정말로 COVID- 99 에 감염되었을 확률은?&lt;/p&gt;

&lt;p&gt;아래의 베이즈 정리 식을 이용하면 쉽게 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;우리가 구하고자 하는 값은 어떤 사람이 질병에 걸렸다고 검진결과가 나왔을 때 정말로 COVID- 99에 감염되었을 확률이고 이는 바로 사후확률이 됩니다.&lt;/p&gt;

&lt;p&gt;이 사후확률은 사전확률인 발병률이 10% * 가능도인 COVID-99 에 실제로 걸렸을 때 검진될 확률은 99% / Evidence 로 구할 수 있습니다.&lt;/p&gt;

&lt;p&gt;Evidence는 다음과 같이 구할 수 있습니다.&lt;br /&gt;
질병에 결렸을 때, 검진될 확률 * 발병률 + 질병에 걸리지 않았을 때 검진될 확률(오진) * 발병률의 부정&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;따라서 위 값을들 조합해 계산해보면 아래와 같습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;만약 오진률 즉, 발병되지 않았는데 검진될 확률이 0.1 = 10%로 올라간다면, Evidence에서의 값이 증가하게 되고 이는 사후 확률을 낮추게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;오진률이 높아짐으로 인해, 질병에 걸렸다고 검진결과가 나왔을 때, 실제로 그사람이 감염되었을 확률이 낮아진다는 것입니다.&lt;/p&gt;

&lt;h3 id=&quot;베이즈-정리를-통한-정보의-갱신&quot;&gt;베이즈 정리를 통한 정보의 갱신&lt;/h3&gt;

&lt;p&gt;베이즈 정리를 통해 새로운 데이터가 들어왔을 때 앞서 계산한 사후확률을 사전확률로 사용하여 갱신된 사후확률을 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;앞서 COVID-99 판정을 받은 사람이 두 번째 검진을 받았을 때도 양성이 나 왔을 때 진짜 COVID-99 에 걸렸을 확률은?&lt;/p&gt;

&lt;p&gt;갱신된 evidence는 다음과 같이 계산 됩니다.&lt;/p&gt;

&lt;p&gt;빨간색 텀이 바로 전 단계에서 계산한 사후확률이고 이것이 이번 단계에서의 사전 확률로 들어가게 되는 것입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그리고 갱신된 사후확률은 다음과 같이 계산될 수 있습니다.&lt;/p&gt;

&lt;p&gt;사전확률이 전 단계에서 계산된 값으로 들어가고 위에서 계산한 evidence값이 분모로 들어가게 됩니다.&lt;br /&gt;
나머지는 원래 계산하던 것과 마찬가지 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;deep-learning-basic&quot;&gt;Deep Learning Basic&lt;/h2&gt;

&lt;p&gt;딥러닝에 대한 소개, 그리고 딥러닝의 역사에 대해 배웁니다.&lt;/p&gt;

&lt;p&gt;CNN(Convolutional neural networks), RNN(Recurrent Neural Networks)와 같은 딥러닝 모델을 공부하기 전에 중요한 요소인 Data, Model, Loss, Optimization algorithms에 대해 배웁니다.&lt;/p&gt;

&lt;h3 id=&quot;what-make-you-a-good-deep-learner-&quot;&gt;What make you a good deep learner ?&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;가지고 있는 아이디어나 논문의 이론을 실제로 구현할 수 있는 능력&lt;/li&gt;
  &lt;li&gt;딥러닝을 공부하고 연구하는데에 필요한 선형대수와 통계학 등..&lt;/li&gt;
  &lt;li&gt;현재 어떤 트렌드가 있고, 어떤 연구결과가 나왔는지 아는 것이 매우 중요하다&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;contents&quot;&gt;Contents&lt;/h3&gt;

&lt;p&gt;앞으로 배우게 될 내용&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Historical Review - 딥러닝의 역사적 리뷰&lt;/li&gt;
  &lt;li&gt;Neural Networks &amp;amp; Multi-Layer Perceptron - 신경망과 다층 퍼셉트론&lt;/li&gt;
  &lt;li&gt;Optimization Methods - 최적화 방법 (드롭아웃, 규제 등)&lt;/li&gt;
  &lt;li&gt;Convolutional Neural Networks - 합성곱 신경망&lt;/li&gt;
  &lt;li&gt;Modern CNN&lt;/li&gt;
  &lt;li&gt;Computer Vision Applications - 컴퓨터 비전 응용&lt;/li&gt;
  &lt;li&gt;Recurrent Neural Networks&lt;/li&gt;
  &lt;li&gt;Transformer&lt;/li&gt;
  &lt;li&gt;Generative Models Part1
10.Generative Models Part2&lt;/li&gt;
&lt;/ol&gt;

&lt;h3 id=&quot;ai--ml--dl&quot;&gt;AI / ML / DL&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;AI - 사람의 지능을 모방하는 것&lt;/li&gt;
  &lt;li&gt;ML - 어떤 문제를 품에 있어, 데이터를 통해 학습하는 분야&lt;/li&gt;
  &lt;li&gt;DL - 사랑의 지능을 모방하면서, 데이터를 통해 학습하고, 그 안에 딥 뉴럴 네트워크를 사용하는 위 두개의 분야 안의 세부적인 분야&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;DL분야를 연구하는 것은 AI분야 전체를 연구한다는 것과는 동치가 아니다.&lt;/p&gt;

&lt;h3 id=&quot;key-components-of-deep-learning&quot;&gt;Key Components of Deep Learning&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The data that the model can learn from&lt;/li&gt;
  &lt;li&gt;The model how to transform the data&lt;/li&gt;
  &lt;li&gt;The loss function that quantifies the badness of the model&lt;/li&gt;
  &lt;li&gt;The algorithm to adjust the parameters to minimize the loss&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Data depend on the type of the problem to solve.&lt;br /&gt;
데이터는 풀고자 하는 문제에 종속적이다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;분류문제에서는 개와 고양이 같은 어떤 종류의 물체인지 분류하고, Semantic Segmentation같은 문제에서는 각 픽셀이 어디에 속하는 부분인지, Detection은 이미지에서 여러 종류의 물체가 있으면 이 물체의 바운더리에 어느 종류의 물체가 있는지 구분합니다.&lt;/p&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;이미지, 텍스트 등이 주어졌을 때, 우리가 알고 싶어하는 문제의 예측값(레이블)을 도출해주는 역할을 한다.&lt;/p&gt;

&lt;p&gt;문제에 따라 여러가지 모델이 존재하고, 성능도 각기 다르다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;loss&quot;&gt;Loss&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;The loss function is a proxy of what we want to achieve.&lt;br /&gt;
문제에서 풀고 이루고자 하는 목적을 달성하기 위한 근사치&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;문제에 따라 적용되는 손실함수의 종류가 달라지게 된다.&lt;/p&gt;

&lt;p&gt;전형적인 선형회귀 같은 문제에서는 예측값과 정답레이블 사이의 거리를 최소화 하기 위해 MSE(최소제곱오차)라든지, 분류문제에서는 여러개의 선택지 중 어느 것이 확률이 제일 높은지 예측하기 위해 CE(교차 엔트로피)같은 손실함수를 적용한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-14.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;optimization-algorithm&quot;&gt;Optimization Algorithm&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/aitech_day11-15.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;퀴즈-정리&quot;&gt;퀴즈 정리&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;다음의 식이 성립하는가?&lt;/p&gt;

    &lt;p&gt;P(A∩B)=P(A)P(B∣A)&lt;/p&gt;

    &lt;p&gt;기본적인 조건부 확률 문제로서, P(B∣A)는 사건 A가 일어났을 때, B가 일어날 확률을 의미합니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;사후확률 (posterior) 은 가능도 (likelihood) 에 반비례하는가?&lt;/p&gt;

    &lt;p&gt;가능도는 사후확률에 비례하고, Evidence가 반비례하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다음의 식이 성립하는가?&lt;/p&gt;

    &lt;p&gt;P(A∣B) = P(A)P(B∣A)/P(B)&lt;/p&gt;

    &lt;p&gt;양변에 P(A)를 곱해주면 식은 성립하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;A가 binary variable일 때, 다음의 식이 성립하는가?&lt;/p&gt;

    &lt;p&gt;P(A∣B) = P(B∣A)P(A) / P(B∣A)P(A)+P(B∣¬A)P(¬A)&lt;/p&gt;

    &lt;p&gt;이항변수일 때의 베이즈 정리의 공식을 묻는 문제였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;모든 변수에 대한 조건부 확률만으로 인과관계를 추론할 수 있는가?&lt;/p&gt;

    &lt;p&gt;인과관계를 추론함에 있어 어떤 중첩효과가 있는지 연구 후에 이를 제거해야 올바른 추론이 가능합니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Mathematics for Artificial Intelligence - Unist 임성철 교수님&lt;/li&gt;
  &lt;li&gt;Deep Learning Basic - 최성준 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">부스트캠프 3주차 Day11 강의를 보고 내용을 정리한 노트입니다.</summary></entry><entry><title type="html">matplotlib</title><link href="http://localhost:4000/development/boostcamp-week2-day10/" rel="alternate" type="text/html" title=" matplotlib" /><published>2021-01-29T00:00:00+09:00</published><updated>2021-01-29T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week2-day10</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week2-day10/">&lt;h1 id=&quot;matplotlib&quot;&gt;matplotlib&lt;/h1&gt;

&lt;p&gt;데이터 분석을 위해서 가장 기초적인 단계로 해당 데이터를 시각화하여 보여줄 수 있는 능력이 필요합니다. 시각화 라이브러리는 파이썬에서만 10개가 넘는등 다양한 라이브러리가 존재합니다. 그 중에서 가장 대중적으로 많이 쓰이는 matplotlib, seaborn 모듈에 대해 알아보겠습니다.&lt;/p&gt;

&lt;h2 id=&quot;matplotlib-1&quot;&gt;matplotlib&lt;/h2&gt;

&lt;p&gt;가장 대중적으로 많이 쓰였고 많은 파이썬 라이브러리의 근간이 되는 matplotlib 입니다. matplotlib는 다른 라이브러리들의 부모 라이브러리로서의 역할을 하고 있다고 표현할정도로 다른 라이브러리들에 많은 영향을 주었습니다. 다소 복잡한 라이브러리 구성으로 인해 최근에는 그 사용 빈도와 대중성이 떨어지고 있으나 여전히 많은 입문자들이 처음 사용해보게 되는 좋은 시각화 라이브러리입니다.&lt;/p&gt;

&lt;h3 id=&quot;matplotlib-overview&quot;&gt;matplotlib overview&lt;/h3&gt;

&lt;p&gt;그래프는 pyplot 객체를 사용하여 데이터를 표시합니다.&lt;br /&gt;
그 다음 pyplot 객체에 그래프들을 쌓은 다음 flush하게 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;plot 메소드는 argument를 kwargs인 키워드로 받기 때문에 고정된 argument가 없어서 alt+tab으로 사전에 어떤 parameter로 구성되어 받는지 확인이 어렵습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그래프는 원래 figure 객체에 생성되는데, pyplot 객체 사용시, 기본 figure에 그래프가 그려집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;figure--axes&quot;&gt;Figure &amp;amp; Axes&lt;/h3&gt;

&lt;p&gt;Matplotlib은 Figure 안에 Axes로 구성이 됩니다.&lt;br /&gt;
Figure 위에 여러 개의 Axes를 생성&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Subplot의 순서는 그리드로 작성됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-color&quot;&gt;set color&lt;/h3&gt;

&lt;p&gt;color 속성을 사용
float: 흑백, rgb color, predefined color 사용 가능&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-line-style&quot;&gt;set line style&lt;/h3&gt;

&lt;p&gt;라인 스타일은 ls 또는 linestyles 속성을 사용함으로써 적용가능 합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-title&quot;&gt;set title&lt;/h3&gt;

&lt;p&gt;pyplot에 title 함수 사용, figure의 subplot별 입력 가능&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;set-legend&quot;&gt;set legend&lt;/h3&gt;

&lt;p&gt;legend 함수로 범례를 표시함, loc 위치 등 속성 지정가능&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;scatter&quot;&gt;scatter&lt;/h3&gt;

&lt;p&gt;scatter 함수를 사용해 그래프를 그릴 수 있습니다.&lt;/p&gt;

&lt;p&gt;scatter 함수는 argument로 data_1, data_2같은 시퀀스 자료형을 받고 있습니다.&lt;br /&gt;
c는 컬러, marker는 모양을 지정하는 속성입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;s : 데이터의 크기를 지정, 데이터의 크기 비교 가능&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;bar-chart&quot;&gt;bar chart&lt;/h3&gt;

&lt;p&gt;bar 함수로 bar chart를 그릴 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;histogram&quot;&gt;histogram&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-14.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;seaborn&quot;&gt;seaborn&lt;/h3&gt;

&lt;p&gt;시각화를 위한 다양한 기능을 손쉽게 사용할 수 있도록 지원합니다. 모든 기능은 matplotlib을 기반으로 제공되어 matplotlib과 상호 호환됩니다. matplotlib의 모든 기능을 사용하면서 손쉽게 사용하고 싶다면 seaborn이 가장 적절한 대안입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;기존 matplotlib에 기본 설정을 추가&lt;/li&gt;
  &lt;li&gt;복잡한그래프를 간단하게 만들 수 있는 wrapper&lt;/li&gt;
  &lt;li&gt;간단한 코드 + 예쁜 결과&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-15.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;matplotlib과 같은 기본적인 plot&lt;/li&gt;
  &lt;li&gt;손쉬운 설정으로 데이터 산출&lt;/li&gt;
  &lt;li&gt;lineplot, scatterplot, countplot 등&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-16.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-17.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/matplotlib&amp;amp;probability_basic-18.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;퀴즈-정리&quot;&gt;퀴즈 정리&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;다음과 같은 표본 $X$ 가 있을 때, $X$ 의 평균을 구하시오 (정수값으로 입력).&lt;/p&gt;

    &lt;p&gt;표본을 모두 더한 다음 갯수로 나눠주면 되는 문제였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다음과 같은 표본 $X$ 가 있을 때, $X$ 의 표본분산을 구하시오 (소수점 첫째자리까지 입력)&lt;/p&gt;

    &lt;p&gt;표본들을 표본평균으로 뺀 값들을 제곱한 다음 전부 더해 [표본 개수 - 1] 로 나눠주면 되는 문제였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;다음과 같은 표본 $X$ 가 있을 때, $X$x 의 표본표준편차를 구하시오 (정수로 입력).&lt;/p&gt;

    &lt;p&gt;위의 표본에서 표본분산을 구해서 제곱근을 취해주면 되는 문제였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;정답 레이블을 one-hot 벡터로 표현한다면 하나의 정답 레이블 벡터의 크기는 1이다?&lt;/p&gt;

    &lt;p&gt;범주의 개수가 하나의 정답 레이블 벡터의 크기가 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;KL(P∥Q)는 KL(Q∥P) 와 같다?&lt;/p&gt;

    &lt;p&gt;쿨백-라이블러 발산의 이산확률변수에서의 경우와 연속확률변수에서의 경우는 다릅니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;pandas - 최성철 교수님&lt;/li&gt;
  &lt;li&gt;Mathematics for Artificial Intelligence - Unist 임성철 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">부스트캠프 2주차 Day10 강의를 보고 내용을 정리한 노트입니다.</summary></entry><entry><title type="html">Pandas / 기초 확률론</title><link href="http://localhost:4000/development/boostcamp-week2-day9/" rel="alternate" type="text/html" title="Pandas / 기초 확률론" /><published>2021-01-28T00:00:00+09:00</published><updated>2021-01-28T22:06:00+09:00</updated><id>http://localhost:4000/development/boostcamp-week2-day9</id><content type="html" xml:base="http://localhost:4000/development/boostcamp-week2-day9/">&lt;h1 id=&quot;판다스--딥러닝에서의-확률론-기본&quot;&gt;판다스 / 딥러닝에서의 확률론 기본&lt;/h1&gt;

&lt;p&gt;판다스는 구조화된 데이터의 처리를 지원하는 Python 라이브러리입니다.&lt;br /&gt;
고성능 array 계산 라이브러리인 numpy와 통합하여, 강력한 “스프레드시트” 처리 기능을 제공하므로 Data science 분야에서 널리 쓰이는 판다스를 알아봅니다.&lt;/p&gt;

&lt;p&gt;저번 포스팅에 이어, pandas 라이브러리 기능에 대해 알아봅니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;딥러닝의 왜 확률론이 필요한지 알아봅니다.&lt;br /&gt;
데이터의 초상화로써 확률분포가 가지는 의미와 이에 따라 분류될 수 있는 이산확률변수, 연속확률변수의 차이점, 확률변수, 조건부확률, 기대값 등에 대해 알아봅니다.&lt;/p&gt;

&lt;h2 id=&quot;pandas---groupby&quot;&gt;pandas - groupby&lt;/h2&gt;

&lt;p&gt;특정 조건에 맞는 데이터들을 묶을 수 있는 기능입니다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SQL groupby 명령어와 같음&lt;/li&gt;
  &lt;li&gt;split -&amp;gt; apply -&amp;gt; combine 과정을 거쳐 연산함&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-1.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;그룹바이 함수에는 &lt;strong&gt;묶음의 기준이 되는 컬럼이 들어가고&lt;/strong&gt; 뒤에 적용하고자 하는 연산에 해당되는 컬럼을 입력합니다.&lt;/p&gt;

&lt;p&gt;결과는 팀을 기준을 한 point들을 모두 합한 시리즈가 출력이 됩니다.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;아래 그림 처럼 한 개 이상의 column을 묶을 수도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-2.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-index&quot;&gt;Hierarchical index&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Groupby 명령의 결과물은 dataframe 입니다.&lt;/li&gt;
  &lt;li&gt;두 개의 column으로 groupby를 할 경우, index가 두개 생성됩니다.&lt;/li&gt;
  &lt;li&gt;아래의 그림에서는 ‘Team’과 ‘Year’의 인덱스가 생성된 모습입니다.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-3.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-index--unstack&quot;&gt;Hierarchical index – unstack()&lt;/h3&gt;

&lt;p&gt;Group으로 묶여진 데이터를 matrix 형태로 전환해주는 기능입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-4.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;hierarchical-index--operations&quot;&gt;Hierarchical index – operations&lt;/h3&gt;

&lt;p&gt;Index level을 기준으로 주어진 기본 연산이 수행 됩니다.&lt;/p&gt;

&lt;p&gt;밑의 그림에서 level 0은 Team이고, level2는 Year 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-5.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;groupby--gropued&quot;&gt;Groupby – gropued&lt;/h3&gt;

&lt;p&gt;Groupby에 의해 Split된 상태를 추출 가능합니다.&lt;/p&gt;

&lt;p&gt;Tuple 형태로 그룹의 key 값 Value값이 추출됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-6.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;특정 key값을 가진 그룹의 정보만 추출도 가능합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-7.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;groupby--aggregation&quot;&gt;Groupby – aggregation&lt;/h3&gt;

&lt;p&gt;추출된 group 정보에 Aggregation이라는 요약된 통계정보를 추출해 줄 수 있는 기능을 적용할 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-8.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;특정 컬럼에 여러개의 function을 Apply 할 수 도 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-9.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;groupby--transformation&quot;&gt;Groupby – transformation&lt;/h3&gt;

&lt;p&gt;Transformation은 Aggregation과 달리 key값 별로 요약된 정보가 아니고 개별 데이터의 변환을 지원합니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-10.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;전체 그룹에서 칼럼 별로 가장 큰 값을 뽑아서, 그 값으로 전체 각각의 그룹의 칼럼 별 데이터를 갱신합니다.&lt;/p&gt;

&lt;h3 id=&quot;merge--concat&quot;&gt;Merge &amp;amp; Concat&lt;/h3&gt;

&lt;p&gt;Merge는 SQL에서 많이 사용하는 Merge와 같은 기능으로써, 두개의 데이터를 하나로 합치는 기능입니다.&lt;/p&gt;

&lt;p&gt;같은 칼럼 네임을 갖고 있는 데이터 프레임이 두개가 있고, ‘subject_id’의 같은 칼럼 값으로 merge를 하면 다음 그림과 같이 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-11.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;합병의 기준이 되는 컬럼의 이름이 다를 땐 ‘left_on’, ‘right_on’ 옵션을 각각 지정해주면 됩니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-12.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;SQL을 하신 분이라면 익숙한 그림입니다.&lt;br /&gt;
Merge는 어느 방법을 쓰느냐에 따라, 다음과 같이 도식화 될 수 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-13.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;merge-example&quot;&gt;Merge example&lt;/h4&gt;

&lt;p&gt;다음은 예제에 사용될 데이터 프레임들 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-14.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;left-join&quot;&gt;left join&lt;/h4&gt;

&lt;p&gt;subject_id가 기준이 되면서, 왼쪽에 있는 모든 데이터도 머지 대상이 됩니다.&lt;br /&gt;
오른쪽 데이터에서 왼쪽 데이터의 칼럼에 해당되지 않는 부분은 NaN값으로 채워집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-15.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;right-join&quot;&gt;right join&lt;/h4&gt;

&lt;p&gt;subject_id가 기준이 되면서, 오른쪽에 있는 모든 데이터도 머지 대상이 됩니다.&lt;br /&gt;
왼쪽 데이터에서 오른쪽 데이터의 칼럼에 해당되지 않는 부분은 NaN값으로 채워집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-16.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h4 id=&quot;fullouter-join&quot;&gt;full(outer) join&lt;/h4&gt;

&lt;p&gt;subject_id가 기준이 되면서, 양쪽에 있는 모든 데이터가 머지 대상이 됩니다.&lt;br /&gt;
왼쪽 데이터에서 오른쪽 데이터의 칼럼에 해당되지 않는 부분 그리고 오른쪽 데이터에서 왼쪽 데이터의 칼럼에 해당되지 않는 부분은 NaN 값으로 채워집니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-17.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;concat&quot;&gt;Concat&lt;/h3&gt;

&lt;p&gt;말그대로, 같은 형태의 데이터를 붙이는 연산작업 입니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-18.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pd.concat([df1, df2])&lt;/code&gt;과 `df1.append(df2) 방식이 있습니다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/pandas2&amp;amp;probability_basic-19.png&quot; alt=&quot;Alt text&quot; width=&quot;600px&quot; class=&quot;align-center&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;딥러닝-확률론&quot;&gt;딥러닝 확률론&lt;/h2&gt;

&lt;p&gt;딥러닝에서 확률론이 왜 필요한가요?&lt;/p&gt;

&lt;p&gt;딥러닝은 확률론 기반의 기계학습 이론에 바탕을 두고 있습니다. 기계학습에서 사용되는 손실함수(loss function)들의 작동 원리는 데이터 공간을 통계적으로 해석해서 유도하게 됩니다. (예측이 틀릴 위험(risk)을 최소화하도록 데이터를 학습하는 원리는 통계적 기계학습의 기본 원리)&lt;/p&gt;

&lt;p&gt;회귀 분석에서 손실함수로 사용되는 L2-노름은 예측오차의 분산을 가장 최소화하는 방향으로 학습하도록 유도합니다.&lt;/p&gt;

&lt;p&gt;분류 문제에서 사용되는 교차엔트로피(cross-entropy)는 모델 예측의 불확실성을 최소화하는 방향으로 학습하도록 유도합니다.&lt;/p&gt;

&lt;p&gt;분산 및 불확실성을 최소화하기 위해서는 측정하는 방법을 알아야하는데, 두 대상을 측정하는 방법을 통계학에서 제공하기 때문에 기계학습을 이해하려면 확률론의 기본 개념을 알아야 합니다.&lt;/p&gt;

&lt;h3 id=&quot;이산확률변수-vs-연속확률변수&quot;&gt;이산확률변수 vs 연속확률변수&lt;/h3&gt;

&lt;p&gt;확률변수는 확률분포 𝒟 에 따라 이산형(discrete)과 연속형(continuous) 확률변수로 구분하게 됩니다.&lt;/p&gt;

&lt;p&gt;이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링합니다.&lt;/p&gt;

&lt;p&gt;$P(X \in A) = \Sigma_{x \in A} P(X = x)$&lt;/p&gt;

&lt;p&gt;연속형 확률변수는 데이터 공간에 정의된 확률변수의 밀도(density) 위에서의 적분을 통해 모델링합니다.&lt;/p&gt;

&lt;p&gt;$P(X \in A) = \int_A P(x)dx$&lt;/p&gt;

&lt;h3 id=&quot;조건부확률과-기계학습&quot;&gt;조건부확률과 기계학습&lt;/h3&gt;

&lt;p&gt;조건부확률 $P(y \mid x)$는 입력변수 $x$ 에 대해 정답이 $y$ 일 확률을 의미합니다.&lt;/p&gt;

&lt;p&gt;로지스틱 회귀에서 사용했던 선형모델과 소프트맥스 함수의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용됩니다.&lt;/p&gt;

&lt;p&gt;분류 문제에서 softmax$(W\phi + b)$은 데이터 $x$ 로부터 추출된 특징패턴 $\phi(x)$ 과 가중치행렬 $W$을 통해 조건부확률 $P(y \mid x)$을 계산합니다.&lt;/p&gt;

&lt;p&gt;회귀 문제의 경우 조건부기대값 $𝔼[y \mid x]$ 을 추정합니다.&lt;/p&gt;

&lt;p&gt;딥러닝은 다층신경망을 사용하여 데이터로부터 특징패턴 $\phi$을 추출합니다&lt;/p&gt;

&lt;h3 id=&quot;기대값이-뭔가요&quot;&gt;기대값이 뭔가요?&lt;/h3&gt;

&lt;p&gt;확률분포가 주어지면 데이터를 분석하는 데 사용 가능한 여러 종류의 통계 적 범함수(statistical functional)를 계산할 수 있습니다.&lt;/p&gt;

&lt;p&gt;기대값(expectation)은 데이터를 대표하는 통계량이면서 동시에 확률분포를 통해 다른 통계적Z 범함수를 계산하는데 사용됩니다.&lt;/p&gt;

&lt;p&gt;$E_{x~P(x)}[f(x)] = \int_X f(x)P(x)dx$, $E_{x~P(X)}[f(x)] = \sum_{x \in X} f(x)P(x)$&lt;/p&gt;

&lt;h1 id=&quot;피어세션-정리&quot;&gt;피어세션 정리&lt;/h1&gt;

&lt;p&gt;오늘은 판다스의 그룹핑에 대한 내용과 딥러닝에서 왜 통계학이 중요하고 사용되는지에 대해 배울 수 있었는데, 오늘 내용 중 통계학 부분의 설명이 좀 난해하고 낯설다는 분이 많이 계서서, 다 같이 어느부분이 이해가 안되고 궁금한지 이야기하는 시간을 가질 수 있었습니다.&lt;/p&gt;

&lt;p&gt;그리고 2주차 들어오면서 수학적인 내용을 다룸에 따라 하루의 마무리인 학습정리를 할 때, 블로그나 다른 매체에 수식을 입력할 수 있는 LaTex라는 문법에 대해 이야기 하였고, 이를 어떻게 하면 효율적으로 배우고 쓸 수 있을지에 대해 이야기해볼 수 있었습니다.&lt;/p&gt;

&lt;h1 id=&quot;퀴즈-정리&quot;&gt;퀴즈 정리&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;이산형 확률변수는 확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링한다?&lt;/p&gt;

    &lt;p&gt;확률변수가 가질 수 있는 경우의 수를 모두 고려하여 확률을 더해서 모델링하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;연속형 확률변수의 한 지점에서의 밀도(density)는 그 자체로 확률값을 가진다?&lt;/p&gt;

    &lt;p&gt;밀도는 누적확률분포에서의 확률이 아니라 변화율을 의미하게 됩니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;몬테카를로 샘플링 방법은 변수 유형 (이산형, 연속형)에 상관없이 사용할 수 있다?&lt;/p&gt;

    &lt;p&gt;상관없이 사용할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각 면이 나올 확률이 균등하고 독립적인 정육면체 주사위를 던진다고 하자. 확률변수 $X$ 는 주사위의 각 면의 숫자를 나타낸다고 할 때 $(X \in {1, 2, 3, 4, 5, 6})$, $X$ 의 기대값을 구하시오 (소수점 첫째자리까지 입력).&lt;/p&gt;

    &lt;p&gt;각 확률변수에 확률함수 값을 곱하여 모두 더하면 되는 문제였습니다.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;각 면이 나올 확률이 균등하고 독립적인 정사면체 주사위를 던진다고 하자. 확률변수 $X$ 는 주사위의 각 면의 숫자를 나타낸다고 할 때 $(X \in {1, 3, 5, 7})$, $X$ 의 분산을 구하시오 (정수값으로 입력).&lt;/p&gt;

    &lt;p&gt;각 확률변수에서 기댓값을 뺀 값들을 각각 제곱하고, 모두 각각의 확률변수와 곱하여 더하면 되는 문제였습니다.&lt;/p&gt;

    &lt;p&gt;또는, 확률변수를 제곱한 값들의 기댓값을에서 기댓값 자체를 제곱한 값을 빼줘도 분산을 구할 수 있습니다.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;pandas - 최성철 교수님&lt;/li&gt;
  &lt;li&gt;Mathematics for Artificial Intelligence - Unist 임성철 교수님&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Gyeong</name></author><category term="AI" /><category term="Python" /><summary type="html">부스트캠프 2주차 Day9 강의를 보고 내용을 정리한 노트입니다.</summary></entry></feed>